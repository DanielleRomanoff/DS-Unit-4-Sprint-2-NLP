{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint Challene Unit 4 Sprint 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielleRomanoff/DS-Unit-4-Sprint-2-NLP/blob/master/Sprint_Challene_Unit_4_Sprint_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QpPcbYew_ttN"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Working with Text Data\n",
        "\n",
        "### Use Python string methods remove irregular whitespace from the following string:"
      ]
    },
    {
      "metadata": {
        "id": "i7w4kFVvEOwq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# All imports in one place whether I need them or not. \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dtotEnsStY5o",
        "outputId": "0b757d3c-311c-4d50-d421-6d0c93aad73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Removing the unnessary white space to make a normal sentence\n",
        "whitespace_string = '\\n\\n  This is a    string   that has  \\n a lot of  extra \\n   whitespace.   '\n",
        "print(whitespace_string.strip())\n",
        "print('---------------')\n",
        "print(' '.join(whitespace_string.split()))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a    string   that has  \n",
            " a lot of  extra \n",
            "   whitespace.\n",
            "---------------\n",
            "This is a string that has a lot of extra whitespace.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Vg1-d2aAsXLn"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Regular Expressions to take the dates in the following .txt file and put them into a dataframe with columns for:\n",
        "\n",
        "[RegEx dates.txt](https://github.com/ryanleeallred/datasets/blob/master/dates.txt)\n",
        "\n",
        "- Day\n",
        "- Month\n",
        "- Year\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KWDiN4C9_0sq",
        "outputId": "8514e5c9-9867-4e9a-fb74-241215d4cffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Bringing in the text file to use and make a page with the required information\n",
        "url = ('https://raw.githubusercontent.com/ryanleeallred/datasets/master/dates.txt')\n",
        "page= requests.get(url)\n",
        "page = page.text\n",
        "page"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'March 8, 2015\\r\\nMarch 15, 2015\\r\\nMarch 22, 2015\\r\\nMarch 29, 2015\\r\\nApril 5, 2015\\r\\nApril 12, 2015\\r\\nApril 19, 2015\\r\\nApril 26, 2015\\r\\nMay 3, 2015\\r\\nMay 10, 2015\\r\\nMay 17, 2015\\r\\nMay 24, 2015\\r\\nMay 31, 2015\\r\\nJune 7, 2015\\r\\nJune 14, 2015\\r\\nJune 21, 2015\\r\\nJune 28, 2015\\r\\nJuly 5, 2015\\r\\nJuly 12, 2015\\r\\nJuly 19, 2015'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "WXT6KrWdEOw9",
        "colab_type": "code",
        "outputId": "49de041b-f83c-496a-97f5-5d6f876243c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Separate the month, day, and year\n",
        "month = re.findall(r\"[a-zA-z]+\", page)\n",
        "year = re.findall(r\"\\d{4}\", page)\n",
        "day = re.findall(r\"\\d+\",page)\n",
        "day = day[::2]\n",
        "# Create the dataframe\n",
        "date = pd.DataFrame({'Month ':month,  'Day ': day,  'Year ':year})\n",
        "date.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Day</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>March</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>April</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Day  Month  Year \n",
              "0    8  March  2015\n",
              "1   15  March  2015\n",
              "2   22  March  2015\n",
              "3   29  March  2015\n",
              "4    5  April  2015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "s4Q0dgoe_uBW"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Bag of Words \n",
        "\n",
        "### Use the twitter sentiment analysis dataset found at this link for the remainder of the Sprint Challenge:\n",
        "\n",
        "[Twitter Sentiment Analysis Dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv)\n",
        "\n",
        " ### Clean and tokenize the documents ensuring the following properties of the text:\n",
        "\n",
        "1) Text should be lowercase.\n",
        "\n",
        "2) Stopwords should be removed.\n",
        "\n",
        "3) Punctuation should be removed.\n",
        "\n",
        "4) Tweets should be tokenized at the word level. \n",
        "\n",
        "(The above don't necessarily need to be completed in that specific order.)\n",
        "\n",
        "### Output some cleaned tweets so that we can see that you made all of the above changes.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1xzdhyTS_3F9",
        "outputId": "09ebccb6-41a4-4ffc-9412-bd477ee26d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Bringing in the data to read\n",
        "url = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/twitter_sentiment_binary.csv'\n",
        "# Read the data\n",
        "twitters = pd.read_csv(url)\n",
        "twitters.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7:30 :O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7:30 :O\n",
              "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
              "4          0           i think mi bf is cheating on me!!!   ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "nK8-vQvRLydY",
        "colab_type": "code",
        "outputId": "c2c1b017-7ba6-4f15-92e2-b452e676ca4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "twitters.isnull().sum()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment        0\n",
              "SentimentText    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "dcqMwP5dEOxK",
        "colab_type": "code",
        "outputId": "31024ed2-a751-455c-edf2-fe7a742267c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Making sentiment data readable\n",
        "twitters['SentimentText'] = twitters['SentimentText'].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
        "twitters.head()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7 30  O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me      ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText\n",
              "0          0                       is so sad for my APL frie...\n",
              "1          0                     I missed the New Moon trail...\n",
              "2          1                            omg its already 7 30  O\n",
              "3          0               Omgaga  Im sooo  im gunna CRy  I ...\n",
              "4          0           i think mi bf is cheating on me      ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "REkVhemLEOxN",
        "colab_type": "code",
        "outputId": "02c0135e-6c7f-4c66-ee3d-cabfed2b55ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Do everything at once. re: from Ryan's notes\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def nltk_tokenize(input):\n",
        "\n",
        "    # word\n",
        "    tokens = word_tokenize(input)\n",
        "    #print(\"Tokens:\", tokens)\n",
        "    #lowercase\n",
        "    lowercase_tokens = [w.lower() for w in tokens]\n",
        "    #print(\"Lowercase:\", lowercase_tokens)\n",
        "    # Strip punctuation\n",
        "    no_punctuation = [x.translate(table) for x in lowercase_tokens]\n",
        "    # Remove words that aren't alphabetic\n",
        "    alphabetic = [word for word in no_punctuation if word.isalpha()]\n",
        "    # Remove stopwords\n",
        "    words = [w for w in alphabetic if not w in stop_words]\n",
        "    return words\n",
        "  \n",
        "\n",
        "# Create a new column with the tokenized data\n",
        "twitters['tokenized'] = twitters['SentimentText'].apply(nltk_tokenize)\n",
        "twitters.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "      <td>[sad, apl, friend]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "      <td>[missed, new, moon, trailer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7 30  O</td>\n",
              "      <td>[omg, already]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
              "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me      ...</td>\n",
              "      <td>[think, mi, bf, cheating, tt]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText  \\\n",
              "0          0                       is so sad for my APL frie...   \n",
              "1          0                     I missed the New Moon trail...   \n",
              "2          1                            omg its already 7 30  O   \n",
              "3          0               Omgaga  Im sooo  im gunna CRy  I ...   \n",
              "4          0           i think mi bf is cheating on me      ...   \n",
              "\n",
              "                                           tokenized  \n",
              "0                                 [sad, apl, friend]  \n",
              "1                       [missed, new, moon, trailer]  \n",
              "2                                     [omg, already]  \n",
              "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...  \n",
              "4                      [think, mi, bf, cheating, tt]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q764vszGqiUh"
      },
      "cell_type": "markdown",
      "source": [
        "### How should TF-IDF scores be interpreted? How are they calculated?\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "e2Ji7BMhqs3M"
      },
      "cell_type": "markdown",
      "source": [
        "*Thie higher the weight value, the more rare the term and vice-versa. The TF-IDF tells us how important or not a word is in a document. Search engines use TF-IDF to find appropriate results for a user's serach.*\n",
        "\n",
        "*TF = (Number of times the term appears in a document) / (Total number of terms in the document)\n",
        "IDF = log(number of documents/ Number of times the term appears in all the documents) *\n",
        "\n",
        "#### **TF-IDF = The TF times the IDF**"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3iUeBKtG_uEK"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3 - Document Classification\n",
        "\n",
        "1) Use Train_Test_Split to create train and test datasets.\n",
        "\n",
        "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
        "\n",
        " - Stretch goal: Use both of the methods that we talked about in class.\n",
        "\n",
        "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
        "\n",
        "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
        "\n",
        " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "XZejK0J2fOk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0a2aa87-c49f-4dcf-af18-68b566b35984"
      },
      "cell_type": "code",
      "source": [
        "# Making vector getting rid of brackets in tokenized\n",
        "tokenized_vector = []\n",
        "\n",
        "for i in twitters['tokenized']:\n",
        "    making_vector = \" \".join(i)\n",
        "    tokenized_vector.append(making_vector)\n",
        "    \n",
        "twitters['tokenized_vector'] = tokenized_vector\n",
        "twitters.head()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>tokenized_vector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is so sad for my APL frie...</td>\n",
              "      <td>[sad, apl, friend]</td>\n",
              "      <td>sad apl friend</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>I missed the New Moon trail...</td>\n",
              "      <td>[missed, new, moon, trailer]</td>\n",
              "      <td>missed new moon trailer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>omg its already 7 30  O</td>\n",
              "      <td>[omg, already]</td>\n",
              "      <td>omg already</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Omgaga  Im sooo  im gunna CRy  I ...</td>\n",
              "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
              "      <td>omgaga im sooo im gunna cry dentist since supo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i think mi bf is cheating on me      ...</td>\n",
              "      <td>[think, mi, bf, cheating, tt]</td>\n",
              "      <td>think mi bf cheating tt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentiment                                      SentimentText  \\\n",
              "0          0                       is so sad for my APL frie...   \n",
              "1          0                     I missed the New Moon trail...   \n",
              "2          1                            omg its already 7 30  O   \n",
              "3          0               Omgaga  Im sooo  im gunna CRy  I ...   \n",
              "4          0           i think mi bf is cheating on me      ...   \n",
              "\n",
              "                                           tokenized  \\\n",
              "0                                 [sad, apl, friend]   \n",
              "1                       [missed, new, moon, trailer]   \n",
              "2                                     [omg, already]   \n",
              "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...   \n",
              "4                      [think, mi, bf, cheating, tt]   \n",
              "\n",
              "                                    tokenized_vector  \n",
              "0                                     sad apl friend  \n",
              "1                            missed new moon trailer  \n",
              "2                                        omg already  \n",
              "3  omgaga im sooo im gunna cry dentist since supo...  \n",
              "4                            think mi bf cheating tt  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TX8OEgUP_3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fb61fc10-90be-489f-bda7-4c4e01f046f2"
      },
      "cell_type": "code",
      "source": [
        "# Assigning X and y values\n",
        "X = twitters.tokenized_vector\n",
        "y = twitters.Sentiment\n",
        "\n",
        "# Splitting data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Confirming correct shapes of data\n",
        "print(\"X_train shape is: \", (X_train.shape))\n",
        "print(\"X_test shape is: \",(X_test.shape))\n",
        "print(\"y_train shape is: \",(y_train.shape))\n",
        "print(\"y_trest shape is: \",(y_test.shape))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape is:  (79991,)\n",
            "X_test shape is:  (19998,)\n",
            "y_train shape is:  (79991,)\n",
            "y_trest shape is:  (19998,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y3UrPwWtajUf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88fcc64e-0979-4ab2-f6ef-0cc04e093248"
      },
      "cell_type": "code",
      "source": [
        "# Count Vectorization on training and testing data\n",
        "vectorizer = CountVectorizer(max_features=100, ngram_range=(1,1), stop_words='english')\n",
        "vectorizer.fit(X_train)\n",
        "train_word_counts = vectorizer.transform(X_train)\n",
        "test_word_counts = vectorizer.transform(X_test)\n",
        "\n",
        "X_train_vectorized = pd.DataFrame(train_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "X_test_vectorized = pd.DataFrame(test_word_counts.toarray(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sleep': 66, 'quot': 60, 'haha': 25, 'love': 44, 'little': 40, 'got': 22, 'dont': 12, 'know': 36, 'lt': 45, 'twitpic': 84, 'wan': 88, 'na': 51, 'hear': 28, 'like': 39, 'week': 92, 'http': 34, 'com': 7, 'time': 78, 'long': 42, 'thing': 75, 'good': 21, 'night': 55, 'way': 91, 'today': 79, 'yes': 99, 'cool': 9, 'day': 10, 'right': 62, 'lol': 41, 'new': 53, 'nice': 54, 'happy': 26, 'sad': 63, 'come': 8, 'yeah': 98, 'think': 76, 'im': 35, 'really': 61, 'want': 89, 'bad': 3, 'miss': 49, 'help': 29, 'amp': 0, 'twitter': 85, 'gon': 20, 'let': 37, 'oh': 56, 'ok': 57, 'thanks': 73, 'bit': 6, 'said': 64, 'work': 96, 'tomorrow': 80, 'hope': 33, 'feel': 13, 'better': 5, 'soon': 67, 'hi': 31, 'make': 46, 'going': 19, 'follow': 14, 'getting': 16, 'need': 52, 'great': 23, 'ya': 97, 'morning': 50, 'people': 58, 'wait': 87, 'thank': 72, 'welcome': 94, 'ur': 86, 'thats': 74, 'life': 38, 'fun': 15, 'sorry': 68, 'look': 43, 'guys': 24, 'girl': 17, 'wish': 95, 'aww': 2, 'tonight': 81, 'tell': 71, 'home': 32, 'sounds': 69, 'thought': 77, 'awesome': 1, 'say': 65, 'hey': 30, 'sure': 70, 'watch': 90, 'best': 4, 'weekend': 93, 'hate': 27, 'days': 11, 'maybe': 48, 'glad': 18, 'try': 82, 'tweet': 83, 'pretty': 59, 'man': 47}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9U_Tavc-aj2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2c06a3f8-a5ab-49a9-fcc7-aa54c6f68c11"
      },
      "cell_type": "code",
      "source": [
        "# Logistic Regression model\n",
        "logistic_reg = LogisticRegression(random_state=42, solver=\"lbfgs\").fit(X_train_vectorized, y_train)\n",
        "\n",
        "train_predictions = logistic_reg.predict(X_train_vectorized)\n",
        "test_predictions = logistic_reg.predict(X_test_vectorized)\n",
        "\n",
        "print('REPORTS: Logistic Regression')\n",
        "print('---------------------')\n",
        "print(f'Train Accuracy: {round(accuracy_score(y_train, train_predictions),4)}')\n",
        "print(f'Test Accuracy: {round(accuracy_score(y_test, test_predictions),4)}')\n",
        "print('---------------------')\n",
        "print(f'Train Roc Auc: {round(roc_auc_score(y_train, train_predictions),4)}')\n",
        "print(f'Test Roc Auc: {round(roc_auc_score(y_test, test_predictions),4)}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REPORTS: Logistic Regression\n",
            "---------------------\n",
            "Train Accuracy: 0.6546\n",
            "Test Accuracy: 0.6501\n",
            "---------------------\n",
            "Train Roc Auc: 0.6244\n",
            "Test Roc Auc: 0.6209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aaxfXVYTaj5x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b0d2d02d-e4e8-4a21-fd65-875557320ac9"
      },
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "rand_for= RandomForestClassifier(n_estimators=200).fit(X_train_vectorized, y_train)\n",
        "\n",
        "train_predictions = rand_for.predict(X_train_vectorized)\n",
        "test_predictions = rand_for.predict(X_test_vectorized)\n",
        "\n",
        "print('REPORTS: Random Forest Classifier')\n",
        "print('---------------------')\n",
        "print(f'Train Accuracy: {round(accuracy_score(y_train, train_predictions),4)}')\n",
        "print(f'Test Accuracy: {round(accuracy_score(y_test, test_predictions),4)}')\n",
        "print('---------------------')\n",
        "print(f'Train Roc Auc: {round(roc_auc_score(y_train, train_predictions),4)}')\n",
        "print(f'Test Roc Auc: {round(roc_auc_score(y_test, test_predictions),4)}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REPORTS: Random Forest Classifier\n",
            "---------------------\n",
            "Train Accuracy: 0.7433\n",
            "Test Accuracy: 0.6364\n",
            "---------------------\n",
            "Train Roc Auc: 0.7194\n",
            "Test Roc Auc: 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yxJ-dC0UakCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3cc05fd0-4173-4efd-b34e-8047aacb7d27"
      },
      "cell_type": "code",
      "source": [
        "# Pipeline Created, Tfidf Vectorizer, Multinomial NB, GridSearch CV\n",
        "stop = stopwords.words('english')\n",
        "nb_tfidf = make_pipeline(TfidfVectorizer(stop_words=stop),\n",
        "                        MultinomialNB())\n",
        "nb_grid_params = [{'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
        "                  'tfidfvectorizer__max_features' : [50, 100, None]}]\n",
        "\n",
        "nb_grid = GridSearchCV(nb_tfidf, nb_grid_params, cv=4)\n",
        "nb_grid.fit(X_train, y_train)\n",
        "\n",
        "print('REPORTS: Pipeline')\n",
        "print('---------------------')\n",
        "print ('Best Parameters:', nb_grid.best_params_)\n",
        "print('---------------------')\n",
        "print ('CV Score', nb_grid.best_score_)\n",
        "print ('Test Score', nb_grid.score(X_test, y_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REPORTS: Pipeline\n",
            "---------------------\n",
            "Best Parameters: {'tfidfvectorizer__max_features': None, 'tfidfvectorizer__ngram_range': (1, 1)}\n",
            "---------------------\n",
            "CV Score 0.7322948831743571\n",
            "Test Score 0.7361736173617361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sorF95UO_uGx"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4 -  Word2Vec\n",
        "\n",
        "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
        "\n",
        "2) Display the 10 words that are most similar to the word \"twitter\""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DYno4d4N-LHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "3924634b-65ed-4f59-fc47-186309eafa77"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "import gensim"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.123)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.123)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PH9jLyBZhdHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f28QxW0QnfJ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized = twitters.tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJoF-V3UhdNR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1955
        },
        "outputId": "668e8d74-3cad-4ef5-dbae-76816cc660f3"
      },
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokenized, min_count=25, window=5, size=250,negative=10)\n",
        "dir(model)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_adapt_by_suffix',\n",
              " '_check_input_data_sanity',\n",
              " '_check_training_sanity',\n",
              " '_clear_post_train',\n",
              " '_do_train_epoch',\n",
              " '_do_train_job',\n",
              " '_get_job_params',\n",
              " '_get_thread_working_mem',\n",
              " '_job_producer',\n",
              " '_load_specials',\n",
              " '_log_epoch_end',\n",
              " '_log_epoch_progress',\n",
              " '_log_progress',\n",
              " '_log_train_end',\n",
              " '_minimize_model',\n",
              " '_raw_word_count',\n",
              " '_save_specials',\n",
              " '_set_train_params',\n",
              " '_smart_save',\n",
              " '_train_epoch',\n",
              " '_train_epoch_corpusfile',\n",
              " '_update_job_params',\n",
              " '_worker_loop',\n",
              " '_worker_loop_corpusfile',\n",
              " 'accuracy',\n",
              " 'alpha',\n",
              " 'batch_words',\n",
              " 'build_vocab',\n",
              " 'build_vocab_from_freq',\n",
              " 'callbacks',\n",
              " 'cbow_mean',\n",
              " 'clear_sims',\n",
              " 'compute_loss',\n",
              " 'corpus_count',\n",
              " 'corpus_total_words',\n",
              " 'cum_table',\n",
              " 'delete_temporary_training_data',\n",
              " 'doesnt_match',\n",
              " 'epochs',\n",
              " 'estimate_memory',\n",
              " 'evaluate_word_pairs',\n",
              " 'get_latest_training_loss',\n",
              " 'hashfxn',\n",
              " 'hs',\n",
              " 'init_sims',\n",
              " 'intersect_word2vec_format',\n",
              " 'iter',\n",
              " 'layer1_size',\n",
              " 'load',\n",
              " 'load_word2vec_format',\n",
              " 'log_accuracy',\n",
              " 'max_final_vocab',\n",
              " 'min_alpha',\n",
              " 'min_alpha_yet_reached',\n",
              " 'min_count',\n",
              " 'model_trimmed_post_training',\n",
              " 'most_similar',\n",
              " 'most_similar_cosmul',\n",
              " 'n_similarity',\n",
              " 'negative',\n",
              " 'ns_exponent',\n",
              " 'predict_output_word',\n",
              " 'random',\n",
              " 'reset_from',\n",
              " 'running_training_loss',\n",
              " 'sample',\n",
              " 'save',\n",
              " 'save_word2vec_format',\n",
              " 'score',\n",
              " 'sg',\n",
              " 'similar_by_vector',\n",
              " 'similar_by_word',\n",
              " 'similarity',\n",
              " 'syn0_lockf',\n",
              " 'syn1',\n",
              " 'syn1neg',\n",
              " 'total_train_time',\n",
              " 'train',\n",
              " 'train_count',\n",
              " 'trainables',\n",
              " 'vector_size',\n",
              " 'vocabulary',\n",
              " 'window',\n",
              " 'wmdistance',\n",
              " 'workers',\n",
              " 'wv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "0Vu8s0kDhdRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d7c29a3f-33c5-4866-9691-29e8994408d4"
      },
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(list(model.wv.vocab))\n",
        "print(len(model.wv.vocab))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=3078, size=250, alpha=0.025)\n",
            "['sad', 'friend', 'missed', 'new', 'moon', 'trailer', 'omg', 'already', 'im', 'sooo', 'gunna', 'cry', 'dentist', 'since', 'get', 'put', 'think', 'mi', 'bf', 'tt', 'worry', 'much', 'chillin', 'sunny', 'work', 'tomorrow', 'tv', 'tonight', 'today', 'miss', 'hmmmm', 'wonder', 'number', 'must', 'positive', 'thanks', 'haters', 'face', 'day', 'weekend', 'sucked', 'far', 'jb', 'isnt', 'showing', 'australia', 'ok', 'thats', 'win', 'lt', 'way', 'feel', 'right', 'man', 'completely', 'useless', 'funny', 'twitter', 'http', 'myloc', 'feeling', 'fine', 'gon', 'na', 'go', 'listen', 'celebrate', 'huge', 'roll', 'thunder', 'scary', 'cut', 'growing', 'well', 'year', 'start', 'happy', 'iran', 'one', 'see', 'cause', 'else', 'following', 'pretty', 'awesome', 'level', 'writing', 'massive', 'blog', 'tweet', 'myspace', 'comp', 'shut', 'lost', 'position', 'headed', 'pull', 'golf', 'place', 'something', 'yeah', 'boring', 'whats', 'wrong', 'please', 'tell', 'wish', 'could', 'spend', 'rest', 'life', 'sat', 'going', 'seriously', 'like', 'shit', 'really', 'want', 'sleep', 'nooo', 'hours', 'dancing', 'art', 'assignment', 'finish', 'goodbye', 'exams', 'hello', 'alcohol', 'realize', 'deep', 'geez', 'give', 'girl', 'warning', 'atleast', 'hate', 'tear', 'live', 'guys', 'wearing', 'jeans', 'cute', 'heels', 'sure', 'meet', 'meat', 'bit', 'ly', 'moving', 'saturday', 'morning', 'need', 'days', 'week', 'dont', 'room', 'sick', 'cant', 'till', 'walk', 'yay', 'great', 'times', 'million', 'pictures', 'type', 'brother', 'msn', 'fucked', 'forever', 'amp', 'babes', 'wrote', 'last', 'got', 'call', 'someone', 'york', 'office', 'tumblr', 'com', 'enough', 'said', 'even', 'say', 'anyways', 'chris', 'chicago', 'health', 'class', 'joke', 'show', 'makes', 'look', 'reality', 'time', 'low', 'shall', 'motivation', 'properly', 'another', 'lakers', 'neither', 'magic', 'fun', 'bathroom', 'clean', 'boom', 'proud', 'congrats', 'though', 'david', 'five', 'end', 'july', 'probably', 'never', 'katie', 'concert', 'friends', 'leaving', 'stupid', 'love', 'ur', 'mom', 'hug', 'harry', 'sunday', 'happiness', 'hand', 'u', 'always', 'booo', 'seen', 'allergies', 'hair', 'taking', 'public', 'hurts', 'jersey', 'first', 'hour', 'night', 'find', 'online', 'fix', 'thought', 'become', 'second', 'choice', 'may', 'friendly', 'lol', 'wan', 'home', 'church', 'wht', 'make', 'pizza', 'guitar', 'p', 'x', 'miley', 'tour', 'wanted', 'mean', 'kid', 'stick', 'head', 'fly', 'away', 'slow', 'tix', 'send', 'sunshine', 'ireland', 'swimming', 'beach', 'would', 'walls', 'white', 'idk', 'wat', 'trust', 'sorry', 'da', 'pain', 'ima', 'take', 'dis', 'luv', 'yall', 'finding', 'wall', 'math', 'brain', 'heads', 'come', 'save', 'bed', 'cough', 'airport', 'gt', 'case', 'emo', 'camp', 'wee', 'bringing', 'human', 'watch', 'world', 'report', 'hope', 'jonas', 'almost', 'jus', 'cried', 'quot', 'smile', 'cuz', 'longest', 'ever', 'ugh', 'let', 'game', 'grrr', 'says', 'phone', 'mobile', 'paying', 'car', 'stolen', 'mother', 'hang', 'girls', 'movie', 'plan', 'guess', 'means', 'two', 'cool', 'oh', 'thank', 'pleased', 'soon', 'talkin', 'lose', 'rip', 'rose', 'back', 'special', 'damn', 'half', 'aww', 'status', 'next', 'ex', 'husband', 'daily', 'entry', 'decided', 'help', 'alot', 'better', 'fuck', 'told', 'drink', 'w', 'gave', 'b', 'brought', 'v', 'sweet', 'bought', 'suit', 'true', 'highly', 'actually', 'favorite', 'character', 'book', 'gd', 'knw', 'getting', 'standing', 'rain', 'sitting', 'top', 'summer', 'sucks', 'went', 'dog', 'ear', 'us', 'still', 'everytime', 'moves', 'picture', 'naked', 'jonasbrothers', 'people', 'www', 'content', 'wide', 'awake', 'awful', 'weather', 'comin', 'hi', 'gay', 'normal', 'started', 'squarespace', 'bad', 'anything', 'susan', 'boyle', 'didnt', 'diversity', 'good', 'believe', 'paint', 'story', 'news', 'error', 'uh', 'iphone', 'os', 'fail', 'php', 'know', 'hungry', 'lets', 'outside', 'eat', 'driving', 'mad', 'men', 'uk', 'female', 'internet', 'starting', 'business', 'women', 'tinyurl', 'truth', 'hiding', 'eyes', 'ï', 'blip', 'fm', 'try', 'turn', 'inside', 'admit', 'literally', 'coldplay', 'tears', 'kids', 'break', 'dads', 'sighs', 'plurk', 'annoyed', 'via', 'ow', 'cries', 'potter', 'comes', 'money', 'mouth', 'cheers', 'sigh', 'move', 'lookin', 'around', 'reason', 'atm', 'little', 'atl', 'ily', 'dies', 'soo', 'watching', 'shopping', 'hopefully', 'best', 'weird', 'many', 'truck', 'race', 'worse', 'f', 'confusing', 'hot', 'shirt', 'north', 'due', 'body', 'line', 'missing', 'design', 'tech', 'event', 'everything', 'hard', 'bye', 'ya', 'amazing', 'national', 'reach', 'enjoy', 'venue', 'shine', 'everywhere', 'everyone', 'train', 'crash', 'dc', 'weeks', 'child', 'tr', 'video', 'â', 'wishing', 'ch', 'staying', 'fabulous', 'happened', 'americanwomannn', 'arm', 'leg', 'wait', 'saw', 'mum', 'fat', 'three', 'laptop', 'twitpic', 'facebook', 'maybe', 'later', 'c', 'making', 'bread', 'bday', 'pants', 'job', 'fml', 'haha', 'r', 'science', 'btw', 'blame', 'school', 'done', 'short', 'thing', 'store', 'thinking', 'less', 'chance', 'orlando', 'wud', 'beautiful', 'experience', 'problems', 'yes', 'j', 'k', 'se', 'future', 'nyc', 'cheese', 'nothing', 'kind', 'sort', 'pick', 'flowers', 'teacher', 'jealous', 'without', 'hospital', 'ã', 'security', 'strong', 'link', 'youtube', 'account', 'single', 'ladies', 'dance', 'sold', 'hoping', 'nervous', 'aim', 'talk', 'followers', 'sugar', 'ray', 'convo', 'keep', 'pork', 'bitches', 'wake', 'early', 'la', 'kay', 'etc', 'bother', 'surgery', 'tho', 'tonite', 'hates', 'app', 'used', 'ka', 'blackberry', 'every', 'pop', 'quite', 'shame', 'mia', 'horny', 'island', 'letting', 'check', 'knowledge', 'referring', 'hey', 'thx', 'mothers', 'knew', 'written', 'feels', 'ball', 'heart', 'badly', 'keeps', 'crashing', 'load', 'plane', 'current', 'dad', 'mood', 'currently', 'blah', 'parking', 'lot', 'fed', 'matters', 'worst', 'super', 'nasty', 'rainy', 'bummed', 'drawing', 'fake', 'play', 'camera', 'settle', 'playing', 'iv', 'drive', 'display', 'photos', 'computer', 'effort', 'bring', 'added', 'release', 'tight', 'response', 'kinda', 'moment', 'sadly', 'hilarious', 'punch', 'n', 'crew', 'worth', 'storms', 'loves', 'god', 'friday', 'connection', 'died', 'song', 'apple', 'card', 'helps', 'huh', 'forgot', 'music', 'exhausted', 'ben', 'cake', 'somebody', 'ish', 'long', 'open', 'door', 'throw', 'pc', 'hell', 'bbc', 'made', 'nice', 'twice', 'brilliant', 'xd', 'lonely', 'blue', 'ill', 'fuckin', 'goin', 'ye', 'html', 'eh', 'leave', 'mexico', 'maths', 'sounds', 'considering', 'library', 'tax', 'things', 'town', 'ñ', 'match', 'baby', 'member', 'family', 'cold', 'goes', 'books', 'results', 'poor', 'lil', 'enjoying', 'big', 'yep', 'epic', 'magazine', 'broke', 'sux', 'wondering', 'hay', 'found', 'boy', 'stay', 'holy', 'iranelection', 'air', 'france', 'nl', 'whole', 'bunch', 'follow', 'hear', 'asking', 'working', 'boss', 'lady', 'series', 'boston', 'played', 'terrible', 'soccer', 'bout', 'angry', 'sry', 'matt', 'couldnt', 'near', 'cell', 'screen', 'dead', 'texts', 'calling', 'ohhhh', 'nobody', 'likes', 'reviews', 'read', 'disappointed', 'expecting', 'old', 'gone', 'minutes', 'boo', 'dvd', 'suggestion', 'looks', 'awwwww', 'ticket', 'sale', 'monday', 'peace', 'losing', 'pressure', 'bless', 'goodnight', 'grandma', 'wonderful', 'cavs', 'soooo', 'point', 'years', 'passed', 'exam', 'write', 'use', 'rule', 'answering', 'student', 'picked', 'yet', 'ta', 'john', 'california', 'heard', 'conan', 'profile', 'pic', 'longer', 'luck', 'sometime', 'party', 'conference', 'avatar', 'figure', 'either', 'brothers', 'album', 'movies', 'anyway', 'hollywood', 'rough', 'wants', 'picking', 'food', 'credit', 'cats', 'palm', 'pre', 'fb', 'chicken', 'double', 'large', 'dinner', 'ask', 'hrs', 'dude', 'breakfast', 'cream', 'upload', 'dam', 'vegas', 'floor', 'blessed', 'couple', 'riding', 'green', 'killing', 'bugs', 'asshole', 'asked', 'problem', 'meeting', 'à', 'alice', 'board', 'tired', 'anyone', 'advice', 'lines', 'ready', 'studio', 'woman', 'understand', 'cars', 'cost', 'dollars', 'e', 'exercise', 'build', 'mass', 'text', 'dying', 'horrible', 'nite', 'dreams', 'craving', 'chocolate', 'pub', 'sister', 'wed', 'visit', 'iâ', 'dirty', 'freaking', 'puppy', 'black', 'glad', 'yummy', 'drinks', 'ones', 'tht', 'serious', 'abt', 'thru', 'fav', 'band', 'mental', 'bits', 'pieces', 'ends', 'sooooo', 'fucking', 'complain', 'hes', 'dogs', 'aswell', 'mmm', 'image', 'mess', 'meee', 'name', 'amy', 'iï', 'kris', 'comments', 'confirmed', 'flight', 'crashed', 'onto', 'kept', 'others', 'delete', 'chill', 'swim', 'source', 'reading', 'til', 'crap', 'looking', 'forward', 'mind', 'ah', 'package', 'ahh', 'ahhh', 'mr', 'indeed', 'rock', 'allow', 'otherwise', 'goodness', 'kitchen', 'cup', 'tea', 'dammit', 'dressed', 'ride', 'arms', 'paris', 'moms', 'bored', 'nightmare', 'coming', 'small', 'non', 'posts', 'words', 'allison', 'sleepy', 'gosh', 'typical', 'lame', 'lmao', 'nap', 'excited', 'feelin', 'min', 'somehow', 'mail', 'alarm', 'clock', 'waking', 'children', 'broken', 'felt', 'depressed', 'stress', 'chest', 'mins', 'father', 'tough', 'ahead', 'free', 'copy', 'difference', 'came', 'wit', 'nose', 'expensive', 'trip', 'parties', 'hella', 'seems', 'busy', 'daughter', 'hands', 'cancer', 'lunch', 'general', 'prepare', 'idea', 'hmm', 'closed', 'em', 'crying', 'sore', 'throat', 'unless', 'yea', 'ive', 'loads', 'empty', 'person', 'parents', 'left', 'french', 'andy', 'ass', 'checked', 'stuff', 'real', 'wooo', 'hehe', 'xxxx', 'pie', 'server', 'change', 'loss', 'bummer', 'season', 'hold', 'email', 'signed', 'spam', 'stop', 'least', 'gets', 'nowhere', 'answer', 'photo', 'space', 'red', 'wings', 'final', 'lead', 'stars', 'tickets', 'germany', 'cyrus', 'shoot', 'post', 'young', 'faith', 'fellow', 'hotel', 'judge', 'related', 'nerd', 'whose', 'house', 'confused', 'talking', 'date', 'thinks', 'g', 'rather', 'usual', 'followfriday', 'writer', 'porn', 'model', 'care', 'knows', 'raining', 'alone', 'apparently', 'meaning', 'likely', 'minute', 'issues', 'annoying', 'shower', 'self', 'argh', 'macbook', 'slowly', 'simple', 'brown', 'honestly', 'nd', 'tweets', 'saying', 'crazy', 'yrs', 'ago', 'ended', 'sleeping', 'shift', 'tmrw', 'grr', 'random', 'mention', 'sense', 'xxx', 'aw', 'toy', 'finally', 'dream', 'lived', 'lolz', 'coffee', 'burn', 'homework', 'telling', 'awww', 'upset', 'eating', 'cali', 'glasses', 'pens', 'timing', 'mommy', 'called', 'fit', 'schedule', 'matter', 'kevin', 'bank', 'update', 'baseball', 'straight', 'afraid', 'doesnt', 'tummy', 'besides', 'graduation', 'word', 'close', 'pushing', 'pa', 'afternoon', 'dj', 'jeff', 'fresh', 'third', 'death', 'comic', 'sis', 'prayers', 'twilight', 'wont', 'replies', 'hoo', 'boys', 'water', 'works', 'deleted', 'star', 'testing', 'thursday', 'sf', 'shortstack', 'smiling', 'asleep', 'dark', 'fingers', 'giving', 'fi', 'system', 'promote', 'site', 'front', 'page', 'places', 'social', 'media', 'universe', 'living', 'cares', 'lots', 'ways', 'share', 'feature', 'mixed', 'hugs', 'angel', 'someday', 'might', 'treat', 'doctors', 'wishes', 'jam', 'points', 'anybody', 'cook', 'aint', 'spanish', 'complicated', 'cast', 'ground', 'wet', 'looked', 'fall', 'wtf', 'bill', 'paid', 'tweetdeck', 'holiday', 'paul', 'fl', 'earlier', 'kick', 'warm', 'fast', 'birthday', 'shots', 'billbeckett', 'plenty', 'pay', 'seeing', 'guy', 'banana', 'totally', 'ftw', 'coz', 'england', 'wednesday', 'buy', 'sims', 'ugly', 'bitch', 'rid', 'seem', 'wats', 'prob', 'breath', 'past', 'power', 'contest', 'also', 'noooo', 'international', 'court', 'traffic', 'begin', 'praying', 'yard', 'ariaajaeger', 'light', 'pure', 'speak', 'quiet', 'frozen', 'disappeared', 'billyraycyrus', 'oi', 'prices', 'afford', 'london', 'cat', 'fan', 'speaking', 'middle', 'charlie', 'celebrity', 'dear', 'together', 'myweakness', 'add', 'kno', 'swear', 'paper', 'hehehe', 'covered', 'unfortunately', 'cancelled', 'trying', 'especially', 'google', 'songs', 'thanx', 'youu', 'danny', 'stopped', 'bag', 'sex', 'bing', 'safe', 'search', 'fans', 'slept', 'late', 'country', 'bloody', 'sky', 'lord', 'gud', 'clouds', 'sending', 'although', 'tweeting', 'fave', 'kool', 'okay', 'voice', 'talent', 'bb', 'chat', 'burnt', 'boyfriend', 'aaron', 'suddenly', 'happens', 'mtv', 'awards', 'join', 'watched', 'club', 'suppose', 'log', 'fixed', 'sell', 'eggs', 'bacon', 'miserable', 'hills', 'finale', 'eye', 'waiting', 'test', 'background', 'delicious', 'strawberry', 'city', 'id', 'l', 'xx', 'al', 'state', 'mark', 'cookies', 'gettin', 'drove', 'hr', 'side', 'sniff', 'dan', 'darn', 'def', 'lyrics', 'definately', 'smh', 'depressing', 'silly', 'apps', 'cos', 'complete', 'fell', 'suck', 'updating', 'clothes', 'bbq', 'bath', 'workout', 'run', 'wife', 'expect', 'remember', 'older', 'emails', 'tattoo', 'forget', 'vote', 'ignore', 'smell', 'skin', 'sun', 'ending', 'metro', 'customer', 'service', 'lack', 'road', 'fact', 'h', 'visiting', 'aussie', 'land', 'evening', 'anymore', 'listening', 'er', 'txt', 'plans', 'lovely', 'compliment', 'thankyou', 'st', 'handle', 'brian', 'full', 'control', 'charge', 'contract', 'april', 'dat', 'jst', 'messing', 'candy', 'machine', 'interesting', 'ny', 'chips', 'downloaded', 'yesterday', 'deserve', 'beauty', 'lesson', 'learned', 'outta', 'eaten', 'steak', 'theyre', 'gona', 'mcfly', 'beat', 'finals', 'offline', 'able', 'often', 'catch', 'son', 'msg', 'drunk', 'ing', 'failed', 'pass', 'tip', 'kicked', 'beyond', 'scared', 'cooler', 'field', 'college', 'managed', 'cheap', 'xo', 'imma', 'pls', 'ppl', 'mission', 'san', 'mexican', 'seemed', 'apt', 'recovery', 'mate', 'flat', 'award', 'shes', 'crush', 'none', 'alive', 'note', 'xoxo', 'students', 'killed', 'flickr', 'mac', 'install', 'user', 'cheer', 'regret', 'month', 'jon', 'kate', 'clear', 'shows', 'arrive', 'web', 'finished', 'bite', 'tend', 'private', 'woot', 'four', 'running', 'jobs', 'behind', 'scene', 'gah', 'clearly', 'chelsea', 'events', 'spring', 'mile', 'issue', 'fight', 'welcome', 'personal', 'mix', 'lay', 'classes', 'stream', 'kiss', 'wow', 'ff', 'list', 'ooh', 'woo', 'liked', 'lazy', 'turned', 'wondered', 'sam', 'um', 'garden', 'eventually', 'hubby', 'evil', 'tooth', 'checking', 'perfectly', 'cooking', 'follower', 'holidays', 'twittering', 'mel', 'twitters', 'arrived', 'easy', 'burned', 'photoshop', 'cd', 'pics', 'stuck', 'ship', 'pool', 'correct', 'total', 'tweetup', 'accounts', 'bro', 'title', 'theme', 'outfit', 'nu', 'presentation', 'sadness', 'changed', 'pack', 'june', 'gutted', 'teh', 'sir', 'stoked', 'pair', 'download', 'cancel', 'yo', 'ran', 'hah', 'listened', 'loud', 'tryin', 'doctor', 'term', 'shot', 'photography', 'apart', 'blow', 'fish', 'stressed', 'happen', 'supernatural', 'mon', 'alex', 'shop', 'ice', 'birds', 'certain', 'passing', 'cleaning', 'messages', 'saved', 'box', 'alright', 'barely', 'hit', 'dunno', 'storm', 'dress', 'canceled', 'website', 'possibly', 'spent', 'gold', 'musical', 'high', 'reply', 'local', 'cloudy', 'goodmorning', 'cha', 'games', 'perfect', 'hill', 'adam', 'anywhere', 'tried', 'exist', 'turning', 'members', 'india', 'definitely', 'bah', 'supposed', 'alas', 'feet', 'hurt', 'trek', 'daddy', 'heading', 'smiles', 'snow', 'turns', 'wasnt', 'painting', 'teach', 'yr', 'followed', 'yum', 'gym', 'updates', 'legs', 'laugh', 'ryan', 'hanging', 'hangover', 'falling', 'bike', 'knowing', 'spending', 'instead', 'cousin', 'headache', 'havent', 'updated', 'havin', 'biggest', 'pulled', 'bright', 'hmmm', 'healthy', 'sexy', 'beer', 'wine', 'belly', 'hee', 'winter', 'hurry', 'history', 'review', 'hm', 'relaxing', 'thai', 'awkward', 'comment', 'latest', 'released', 'counting', 'delayed', 'taylor', 'ruined', 'successful', 'shooting', 'needs', 'fire', 'woke', 'soul', 'pages', 'calls', 'therapy', 'ipod', 'touch', 'yah', 'sometimes', 'police', 'battery', 'ca', 'lucky', 'meds', 'horse', 'possible', 'demand', 'stand', 'starts', 'loved', 'foot', 'whilst', 'doin', 'agree', 'ew', 'wise', 'stomach', 'ate', 'deal', 'singing', 'holly', 'stayed', 'beth', 'dislike', 'letter', 'hero', 'loving', 'skype', 'animal', 'everyday', 'ha', 'mo', 'question', 'view', 'fear', 'comfort', 'gig', 'nearly', 'months', 'nxt', 'episode', 'somethin', 'watchin', 'lightning', 'wind', 'babe', 'wild', 'oil', 'rub', 'freakin', 'bull', 'ache', 'invited', 'part', 'dm', 'blonde', 'booked', 'price', 'tag', 'mountain', 'bones', 'swine', 'flu', 'nights', 'bus', 'dnt', 'caught', 'stole', 'hung', 'fault', 'acting', 'impressed', 'youre', 'hawaii', 'starbucks', 'worried', 'meant', 'borrow', 'cash', 'session', 'die', 'officially', 'flying', 'solo', 'team', 'sports', 'lives', 'rofl', 'slightly', 'co', 'tennis', 'careful', 'bay', 'revision', 'shirts', 'crappy', 'putting', 'bio', 'english', 'feedback', 'somewhere', 'steal', 'atlanta', 'mall', 'breaking', 'british', 'theres', 'diego', 'area', 'square', 'location', 'tune', 'learn', 'sit', 'corner', 'net', 'dumb', 'met', 'performance', 'sandwich', 'chinese', 'hole', 'computers', 'amazon', 'within', 'budget', 'bug', 'blows', 'balls', 'ouch', 'episodes', 'files', 'lately', 'beginning', 'insomnia', 'midnight', 'sing', 'tuesday', 'sushi', 'sound', 'fruit', 'bottle', 'realized', 'interest', 'buddy', 'opened', 'posted', 'taken', 'catching', 'easily', 'reminded', 'click', 'progress', 'ol', 'uni', 'company', 'round', 'bell', 'yfrog', 'ashley', 'project', 'ridiculous', 'cinema', 'hahaa', 'sum', 'wedding', 'decide', 'freak', 'fox', 'set', 'dang', 'guessing', 'salad', 'gotten', 'dating', 'saving', 'grew', 'demi', 'mee', 'itunes', 'teeth', 'track', 'skills', 'manager', 'plays', 'soup', 'filled', 'proper', 'diet', 'shake', 'ankle', 'group', 'shaun', 'bradie', 'gorgeous', 'fest', 'jokes', 'spell', 'ring', 'switch', 'fair', 'bc', 'bby', 'direct', 'wars', 'recent', 'desk', 'action', 'rite', 'study', 'tests', 'button', 'miles', 'justin', 'texting', 'hahaha', 'breaks', 'bet', 'winning', 'message', 'everybody', 'philly', 'canada', 'mileycyrus', 'row', 'tis', 'shitty', 'scotland', 'faster', 'le', 'excitement', 'film', 'unfair', 'brings', 'pity', 'apartment', 'wished', 'services', 'humor', 'escape', 'key', 'favourite', 'detroit', 'score', 'ohh', 'blood', 'ed', 'avoid', 'wut', 'exact', 'whether', 'museum', 'exciting', 'ms', 'peter', 'wins', 'bob', 'rich', 'nope', 'states', 'ep', 'course', 'mike', 'odd', 'toast', 'enter', 'memories', 'sarah', 'ako', 'king', 'th', 'ng', 'cricket', 'europe', 'married', 'planned', 'ooo', 'excellent', 'pro', 'leaves', 'fantastic', 'attack', 'edit', 'august', 'crossed', 'finishing', 'essay', 'secret', 'rubbish', 'plus', 'nicer', 'radio', 'emailed', 'becoming', 'sharing', 'taste', 'ideas', 'print', 'fancy', 'revise', 'version', 'present', 'wasted', 'jk', 'christmas', 'ahhhh', 'famous', 'easier', 'guilty', 'ii', 'damnit', 'offer', 'reasons', 'andrew', 'piece', 'bar', 'code', 'exactly', 'rocks', 'firefox', 'flash', 'pray', 'sa', 'ko', 'entire', 'fashion', 'wana', 'quick', 'con', 'ebay', 'girlfriend', 'gmail', 'peeps', 'bk', 'joe', 'twit', 'section', 'lauren', 'mouse', 'tweeps', 'different', 'bottom', 'pissed', 'relationship', 'pm', 'law', 'houston', 'nooooo', 'uncle', 'mama', 'partying', 'color', 'street', 'laundry', 'tu', 'goood', 'extremely', 'doll', 'accident', 'beers', 'dallas', 'whatever', 'period', 'interview', 'shoes', 'que', 'un', 'grace', 'gross', 'prom', 'inaperfectworld', 'block', 'wicked', 'xp', 'nick', 'chair', 'weak', 'z', 'gf', 'curious', 'lee', 'vids', 'wanting', 'signing', 'hav', 'chick', 'split', 'lmfao', 'success', 'tool', 'ordered', 'available', 'quality', 'fam', 'community', 'hide', 'babies', 'hahah', 'target', 'anniversary', 'juice', 'closer', 'heat', 'gives', 'tan', 'trouble', 'glass', 'downtown', 'heavy', 'musicmonday', 'flip', 'wordpress', 'bella', 'luckily', 'ac', 'fever', 'kill', 'addicted', 'ears', 'awe', 'harder', 'sent', 'request', 'appear', 'data', 'runs', 'connect', 'return', 'queen', 'except', 'america', 'toronto', 'hannah', 'took', 'hardly', 'support', 'memory', 'bobby', 'dr', 'laughing', 'wear', 'pink', 'jacket', 'aha', 'parade', 'grab', 'sayin', 'disney', 'received', 'west', 'pet', 'mini', 'tom', 'dropped', 'grand', 'similar', 'jerk', 'depends', 'walked', 'killer', 'center', 'spymaster', 'spain', 'tony', 'dare', 'decent', 'mine', 'pancakes', 'layout', 'typing', 'screwed', 'pan', 'italy', 'idiot', 'sweetheart', 'loser', 'manage', 'ruin', 'quickly', 'messed', 'sign', 'solution', 'helping', 'anytime', 'finger', 'vip', 'grad', 'fab', 'decision', 'fighting', 'questions', 'joined', 'tooo', 'step', 'packing', 'puts', 'greatest', 'given', 'florida', 'seattle', 'survive', 'blast', 'wouldnt', 'remix', 'station', 'spot', 'gurl', 'insane', 'rules', 'mondays', 'couch', 'gun', 'sauce', 'thingy', 'studying', 'worked', 'bird', 'form', 'thumbs', 'original', 'settings', 'hat', 'sisters', 'named', 'posting', 'details', 'bff', 'hv', 'cousins', 'remind', 'el', 'de', 'yoga', 'meh', 'michael', 'folks', 'usually', 'heck', 'lights', 'asap', 'noise', 'upon', 'happening', 'org', 'disappointing', 'ups', 'quit', 'bunny', 'kitty', 'dry', 'mmmm', 'feed', 'hated', 'along', 'whore', 'colour', 'upgrade', 'rains', 'count', 'esp', 'edward', 'war', 'clue', 'based', 'excuse', 'awhile', 'shoulder', 'swift', 'south', 'milk', 'ohhh', 'fyi', 'jack', 'record', 'youuu', 'plz', 'thnx', 'bear', 'zombie', 'complaining', 'bigger', 'role', 'marathon', 'coke', 'tiny', 'football', 'max', 'chuck', 'cutting', 'starving', 'promise', 'uploaded', 'speed', 'mode', 'blocked', 'comedy', 'apply', 'mms', 'limit', 'changes', 'figured', 'mornings', 'names', 'films', 'tee', 'regular', 'kisses', 'shiny', 'interested', 'hint', 'conversation', 'cable', 'personally', 'videos', 'lessons', 'nuts', 'jimmy', 'nowadays', 'wii', 'relax', 'owe', 'lie', 'vodka', 'omfg', 'boat', 'restaurant', 'heyyy', 'drinking', 'rob', 'sorta', 'attend', 'joy', 'tons', 'buying', 'asian', 'mary', 'van', 'continue', 'mrs', 'walking', 'address', 'ping', 'vacation', 'cooked', 'brand', 'expected', 'mid', 'effing', 'threw', 'james', 'inspiration', 'liking', 'tree', 'orange', 'imagine', 'phones', 'across', 'il', 'obviously', 'thoughts', 'mistake', 'loose', 'create', 'deserved', 'jess', 'q', 'info', 'cafe', 'however', 'october', 'quiz', 'ash', 'hun', 'cutie', 'tunes', 'prefer', 'screaming', 'arent', 'princess', 'shoulda', 'edge', 'suspect', 'drag', 'í', 'forgotten', 'sales', 'stories', 'nails', 'willing', 'ahaha', 'act', 'pretend', 'hurting', 'emily', 'smoke', 'painful', 'ten', 'srsly', 'seconds', 'attempt', 'worries', 'dun', 'allowed', 'training', 'strange', 'miami', 'opening', 'kelly', 'rent', 'numbers', 'market', 'alike', 'notice', 'rice', 'japanese', 'pocket', 'result', 'yu', 'amanda', 'bee', 'festival', 'chatting', 'failure', 'grown', 'nicole', 'woohoo', 'tweeted', 'kicking', 'entertaining', 'choose', 'recently', 'jen', 'host', 'noticed', 'hearing', 'keeping', 'major', 'knee', 'truly', 'shout', 'sooner', 'including', 'energy', 'poem', 'notes', 'seats', 'extra', 'tastes', 'window', 'dick', 'age', 'hits', 'bands', 'seat', 'mag', 'ang', 'jump', 'poo', 'remembered', 'darling', 'joining', 'toooo', 'misses', 'rush', 'hasnt', 'noo', 'sea', 'ð', 'feelings', 'minus', 'chip', 'waste', 'brad', 'six', 'wisdom', 'driver', 'size', 'invite', 'official', 'congratulations', 'earth', 'soooooo', 'wash', 'philippines', 'rockin', 'practice', 'order', 'drop', 'yup', 'frm', 'insurance', 'heaps', 'neck', 'moved', 'bsb', 'drama', 'en', 'absolutely', 'cookie', 'commercial', 'theatre', 'vid', 'washing', 'perform', 'characters', 'impossible', 'younger', 'according', 'higher', 'focus', 'ø', 'ù', 'distance', 'learning', 'program', 'keys', 'popular', 'yahoo', 'male', 'marsiscoming', 'genius', 'rate', 'oops', 'american', 'fill', 'using', 'hahahaha', 'per', 'doubt', 'aka', 'awsome', 'article', 'throwing', 'park', 'retweet', 'knock', 'bradiewebbstack', 'beta', 'players', 'force', 'trending', 'andyclemmensen', 'shaundiviney', 'shaunjumpnow', 'alan', 'audio', 'lovin', 'dig', 'zoo', 'quote', 'degrees', 'actual', 'sms', 'realised', 'reminds', 'links', 'dave', 'robert', 'virtual', 'attention', 'auto', 'massage', 'bgt', 'winner', 'career', 'podcast', 'known', 'weight', 'breathe', 'thinkin', 'table', 'holding', 'example', 'suggestions', 'images', 'bts', 'recipe', 'trend', 'nc', 'topic', 'pleasure', 'topics', 'texas', 'classic', 'style', 'fo', 'central', 'delongeday', 'ignoring', 'adding', 'vs', 'software', 'geek', 'beans', 'grow', 'screw', 'nothin', 'japan', 'panic', 'bud', 'sydney', 'repeat', 'booth', 'building', 'dontyouhate', 'items', 'bomb', 'range', 'waves', 'gas', 'parts', 'kings', 'selling', 'heaven', 'obama', 'changing', 'erm', 'cc', 'singer', 'stage', 'voted', 'realise', 'german', 'tweetie', 'es', 'nicely', 'girly', 'creepy', 'plug', 'travel', 'sorted', 'xbox', 'ty', 'yellow', 'september', 'gift', 'nigga', 'channel', 'blink', 'surprise', 'crack', 'britney', 'legal', 'wen', 'needed', 'poster', 'useful', 'appreciate', 'teaching', 'hiya', 'ho', 'smart', 'jake', 'yeahh', 'common', 'ages', 'eu', 'metal', 'homie', 'lover', 'talked', 'microsoft', 'shape', 'access', 'flylady', 'downloading', 'packed', 'cards', 'lo', 'cherry', 'wave', 'sites', 'talented', 'grass', 'mostly', 'born', 'lake', 'information', 'bass', 'whoa', 'consider', 'beverleyknight', 'amandaholden', 'follows', 'competition', 'process', 'sweden', 'whenever', 'nature', 'stock', 'demo', 'takes', 'levels', 'windows', 'east', 'coast', 'spirit', 'contact', 'considered', 'hahahah', 'adorable', 'voting', 'unfollow', 'thread', 'spelling', 'amandapalmer', 'goodsex', 'ðµñ', 'ðµ', 'butt', 'smells', 'sausages', 'hashtag', 'ie', 'haveyouever', 'ä', 'accept', 'users', 'nom', 'hitting', 'soft', 'ooooh', 'george', 'keyboard', 'theory', 'probs', 'situation', 'hoppusday', 'main', 'tryna', 'surprised', 'minds', 'push', 'option', 'concerts', 'pulling', 'cross', 'lying', 'mornin', 'forum', 'iremember', 'cont', 'lips', 'hd', 'important', 'receive', 'heh', 'steve', 'ps', 'incredible', 'preview', 'cover', 'coverage', 'boots', 'hahahahaha', 'bills', 'ashton', 'impressive', 'tips', 'tha', 'respond', 'appreciated', 'spare', 'reminding', 'china', 'recommend', 'several', 'helpful', 'ashleyltmsyf', 'artist', 'inspiring', 'sweetie', 'cupcakes', 'crowd', 'mentioned', 'rise', 'respect', 'professional', 'italian', 'ontd', 'honey', 'envy', 'simply', 'challenge', 'idol', 'jesus', 'joking', 'sets', 'moments', 'reference', 'planning', 'fri', 'dates', 'replied', 'texted', 'peanut', 'butter', 'seb', 'sway', 'usa', 'creative', 'kidding', 'grade', 'singapore', 'sings', 'purchase', 'amount', 'stores', 'ad', 'showed', 'tape', 'sounded', 'twin', 'lies', 'sweets', 'workin', 'previous', 'plugin', 'rude', 'suggest', 'woah', 'ace', 'duh', 'recommendation', 'player', 'network', 'removed', 'file', 'props', 'spread', 'charlieskies', 'animals', 'explain', 'aye', 'terms', 'å', 'launch', 'weekends', 'ì', 'yeh', 'nz', 'cheaper', 'helped', 'nah', 'url', 'dope', 'yawn', 'replace', 'thurs', 'blogs', 'terminator', 'surely', 'brooke', 'trick', 'te', 'di', 'trade', 'difficult', 'scare', 'shortly', 'makeup', 'productive', 'basically', 'journey', 'prolly', 'actor', 'doc', 'umm', 'retarded', 'accent', 'calm', 'yer', 'fool', 'amen', 'enjoyed', 'agentm', 'effect', 'press', 'grey', 'products', 'clip', 'oo', 'tells', 'purple', 'brazil', 'hence', 'patience', 'thin', 'answered', 'laughed', 'hai', 'asia', 'deck', 'aunt', 'involved', 'fully', 'britneyspears', 'backstreetboys', 'passes', 'marketing', 'lemon', 'ðºð', 'perhaps', 'language', 'opinion', 'ali', 'dean', 'papers', 'awh', 'lem', 'jim', 'linux', 'quotes', 'blank', 'recording', 'buzzedition', 'ton', 'meal', 'alyssa', 'hook', 'nevermind', 'pix', 'ink', 'username', 'tube', 'particular', 'guest', 'melbourne', 'din', 'billy', 'camping', 'clever', 'resist', 'answers', 'obsessed', 'lang', 'snap', 'gear', 'convince', 'ë', 'ê', 'included', 'cupcake', 'phew', 'tempted', 'calvinharris', 'uses', 'client', 'license', 'yawns', 'brianmcnugget', 'ooc', 'cam', 'agreed', 'bt', 'anoopdoggdesai', 'amber', 'ashleytisdale', 'logo', 'colors', 'awwww', 'searching', 'built', 'replying', 'ankitagaba', 'oooo', 'ahah', 'normally', 'honest', 'options', 'oooh', 'sho', 'rarely', 'noundiessunday', 'reckon', 'goodluck', 'si', 'akelaa', 'applecored', 'aussiecynic', 'bobbyllew', 'boykillboy', 'hon', 'ditto', 'collection', 'ummm', 'pets', 'atebits', 'heyy', 'yikes', 'hows', 'certainly', 'tues', 'naw', 'corruptedangel', 'crc', 'dvr', 'hayles', 'girlie', 'ikr', 'yess', 'tweeteradder', 'coollike', 'tweeterfollow', 'ba', 'alancarr', 'alyssamilano', 'anna', 'naman', 'yesss', 'angela', 'agentbooth', 'colorblindfish', 'iamsoannoyed', 'aceybongos', 'sd', 'adammshankman', 'adriennebailon', 'ahj', 'crystalchappell', 'alauderdale', 'alexalltimelow', 'alexandramusic', 'alisweeney', 'amberbenson', 'alohaarleen', 'aloliver', 'alyankovic', 'amalinaaa', 'amandabynes', 'amazingphil', 'amazingphoebe', 'amieewhitney', 'anaggh', 'andrewdearling', 'androidtomato', 'andyroddick', 'arsenalsarah', 'î', 'aplusk', 'crucifire', 'ashsimpsonwentz', 'astynes', 'aubreyoday', 'aulia', 'aussiemcflyfan', 'bclub', 'babblingbrookie', 'babygirlparis', 'whatsmysection', 'blokeslib', 'bobbyedner', 'brodyjenner', 'crazytwism', 'buckhollywood', 'butadream', 'cadistra', 'calebftsk', 'cassieventura', 'chelseaplayboy', 'chesterbe', 'chiniehdiaz', 'chrisdaughtry', 'chrisbrogan', 'chriscornell', 'chrisdjmoyles', 'chrishasboobs', 'comeagainjen', 'comedyqueen', 'crunchyk', 'csiprintchick']\n",
            "3078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ND2M5p6ehdT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3fba7402-7559-46da-f8a0-b131bdac713d"
      },
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('twitter', topn=10)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dm', 0.843769371509552),\n",
              " ('facebook', 0.815216064453125),\n",
              " ('updates', 0.809754490852356),\n",
              " ('info', 0.8017176389694214),\n",
              " ('sent', 0.7987399101257324),\n",
              " ('email', 0.7963929772377014),\n",
              " ('link', 0.7949679493904114),\n",
              " ('message', 0.7924069762229919),\n",
              " ('comment', 0.7835289835929871),\n",
              " ('list', 0.7774643301963806)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "WKVBVNPNpvI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "351884f8-7bed-48b8-9abd-d4c97cd663bc"
      },
      "cell_type": "code",
      "source": [
        "#  Try more words\n",
        "model.wv.most_similar('omg')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('xd', 0.8559877872467041),\n",
              " ('soo', 0.8425740003585815),\n",
              " ('hahaha', 0.8268434405326843),\n",
              " ('sooo', 0.8106974959373474),\n",
              " ('lmao', 0.8084343671798706),\n",
              " ('yea', 0.807371199131012),\n",
              " ('dude', 0.8039300441741943),\n",
              " ('wat', 0.7861906886100769),\n",
              " ('jk', 0.7763631939888),\n",
              " ('dat', 0.7757686972618103)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "metadata": {
        "id": "326nbVX-pvW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a51f3d53-de2a-4611-dfd4-97706ccb04a1"
      },
      "cell_type": "code",
      "source": [
        "# Try more words\n",
        "model.wv.most_similar('dog')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.9158025979995728),\n",
              " ('knee', 0.9088286757469177),\n",
              " ('situation', 0.9078789353370667),\n",
              " ('crying', 0.8926919102668762),\n",
              " ('holy', 0.8798405528068542),\n",
              " ('milk', 0.8762333393096924),\n",
              " ('accident', 0.8755646347999573),\n",
              " ('darn', 0.8735130429267883),\n",
              " ('cats', 0.8711155652999878),\n",
              " ('felt', 0.8703235387802124)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}