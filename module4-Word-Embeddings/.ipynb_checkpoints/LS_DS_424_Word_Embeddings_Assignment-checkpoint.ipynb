{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASfGeMfI6Kgs"
   },
   "source": [
    "### Use Word2Vec to train your own model on a dataset.\n",
    "\n",
    "1) **Optional** - Find your own dataset of documents to train you model on. You are going to need a lot of data, so it's probably not realistic to scrape data for this assignment given the time constraints that we're working under. Try to find a dataset that has > 5000 documents.\n",
    "\n",
    "- If you can't find a dataset to use try this one: <https://www.kaggle.com/c/quora-question-pairs>\n",
    "\n",
    "2) Clean/Tokenize the documents.\n",
    "\n",
    "3) Vectorize the model using Word2Vec and explore the results using each of the following at least one time:\n",
    "\n",
    "- your_model.wv.most_similar()\n",
    "- your_model.wv.similarity()\n",
    "- your_model.wv.doesn't_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "5pMqZFKFvliE",
    "outputId": "eb3edc59-e5ed-4bfc-df19-3a3edfc7d59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/ed/70ec5b2601d21e1a3258228d8da563f94ffc86e84cd8a7f600da0c6ebad8/gensim-3.7.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 410kB/s \n",
      "\u001b[?25hCollecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 1.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/danielleromanoff/.local/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: requests in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/71/35e2fb5cee31d846f1139913cbfcdd9dafa49449b33ce712e8e81b70eb81/boto3-1.9.125-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 8.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.4MB/s \n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.125 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/e6/8f50d5442cef835bc83a0555d23305b9210f20eac62990d3ec3911b591d9/botocore-1.12.125-py2.py3-none-any.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 2.1MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.125->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.125->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/danielleromanoff/Library/Caches/pip/wheels/f7/a6/ff/9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/danielleromanoff/Library/Caches/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.125 botocore-1.12.125 bz2file-0.98 gensim-3.7.1 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4524
    },
    "colab_type": "code",
    "id": "M4obR_by1PA2",
    "outputId": "514518b4-9e28-46b4-fc0a-f10270233e79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy5lYo4K8wEy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danielleromanoff/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'today', \"'s\", 'economic', 'upheavals', ',', 'downsizing', ',', 'layoff', ',', 'merger', 'and', 'bankruptcies', 'have', 'cost', 'hundreds', 'of', 'thousands', 'of', 'workers', 'their', 'jobs', '.', 'Millions', 'more', 'have', 'been', 'shifted', 'to', 'unfamiliar', 'tasks', 'within', 'their', 'companies', 'and', 'wonder', 'how', 'much', 'longer', 'they', 'will', 'be', 'employed', '.', 'Adding', 'to', 'the', 'pressures', 'that', 'workers', 'face', 'are', 'new', 'bosses', ',', 'computer', 'surveillance', 'of', 'production', ',', 'fewer', 'health', 'and', 'retirement', 'benefits', ',', 'and', 'the'], ['feeling', 'they', 'have', 'to', 'work', 'longer', 'and', 'harder', 'just', 'to', 'maintain', 'their', 'current', 'economic', 'status', '.', 'Workers', 'at', 'every', 'level', 'are', 'experiencing', 'increased', 'tension', 'and', 'uncertainty', ',', 'and', 'are', 'updating', 'their', 'resumes', '.', 'The', 'loss', 'of', 'a', 'job', 'can', 'be', 'devastating', ',', 'putting', 'unemployed', 'workers', 'at', 'risk', 'for', 'physical', 'illness', ',', 'marital', 'strain', ',', 'anxiety', ',', 'depression', 'and', 'even', 'suicide', '.'], ['Loss', 'of', 'a', 'job', 'affects', 'every', 'part', 'of', 'life', ',', 'from', 'what', 'time', 'you', 'get', 'up', 'in', 'the', 'morning', ',', 'to', 'whom', 'you', 'see', 'and', 'what', 'you', 'can', 'afford', 'to', 'do', '.', 'Until', 'the', 'transition', 'is', 'made', 'to', 'a', 'new', 'position', ',', 'stress', 'is', 'chronic', '.'], ['A', 'feeling', 'of', 'powerlessness', 'is', 'a', 'universal', 'cause', 'of', 'job', 'stress', '.', 'When', 'you', 'feel', 'powerless', ',', 'you', \"'re\", 'prey', 'to', 'depression', \"'s\", 'traveling', 'companions', ',', 'helplessness', 'and', 'hopelessness', '.', 'You', 'do', \"n't\", 'alter', 'or', 'avoid', 'the', 'situation', 'because', 'you', 'feel', 'nothing', 'can', 'be', 'done', '.'], ['Secretaries', ',', 'waitresses', ',', 'middle', 'managers', ',', 'police', 'officers', ',', 'editors', 'and', 'medical', 'interns', 'are', 'among', 'those', 'with', 'the', 'most', 'highly', 'stressed', 'occupations', 'marked', 'by', 'the', 'need', 'to', 'respond', 'to', 'others', \"'\", 'demands', 'and', 'timetables', ',', 'with', 'little', 'control', 'over', 'events', '.', 'Common', 'to', 'this', 'job', 'situation', 'are', 'complaints', 'of', 'too', 'much', 'responsibility', 'and', 'too', 'little', 'authority', ',', 'unfair', 'labor', 'practices', 'and', 'inadequate', 'job', 'descriptions', '.'], ['Employees', 'can', 'counteract', 'these', 'pressures', 'through', 'workers', \"'\", 'unions', 'or', 'other', 'organizations', ',', 'grievance', 'or', 'personnel', 'offices', 'or', ',', 'more', 'commonly', ',', 'by', 'direct', 'negotiations', 'with', 'their', 'immediate', 'supervisors', '.'], ['Every', 'employee', 'should', 'have', 'a', 'specific', ',', 'written', 'job', 'description', '.', 'Simply', 'negotiating', 'one', 'does', 'more', 'to', 'dispel', 'a', 'sense', 'of', 'powerlessness', 'than', 'anything', 'else', 'we', 'know', '.', 'It', 'is', 'a', 'contract', 'that', 'you', 'help', 'write', '.', 'You', 'can', 'object', 'to', 'what', 'and', 'insist', 'on', 'what', 'you', 'do', 'want', '.', 'If', 'there', 'is', 'a', 'compromise', ',', 'it', \"'s\", 'because', 'you', 'agreed', 'to', 'it', '.', 'With', 'a', 'clear', 'job', 'description', ',', 'your', 'expectations', 'are', 'spelled', 'out', ',', 'as', 'are', 'your', 'boss', \"'s\", '.'], ['A', 'good', 'job', 'description', 'is', 'time', 'limited', '.', 'Set', 'a', 'specific', 'date', 'for', 'a', 'review', 'and', 'revision', 'based', 'on', 'your', 'mutual', 'experience', 'with', 'this', 'initial', 'job', 'description', '.', 'If', 'you', 'and', 'your', 'boss', 'ca', \"n't\", 'agree', 'on', 'what', 'your', 'job', 'description', 'should', 'be', ',', 'look', 'for', 'another', 'job', ',', 'either', 'within', 'the', 'same', 'company', 'or', 'outside', '.', 'Even', 'in', 'these', 'tough', 'economic', 'times', ',', 'it', 'is', 'important', 'that', 'your', 'job', 'be', 'a', 'source', 'of', 'satisfaction', 'and', 'respect', '.']]\n"
     ]
    }
   ],
   "source": [
    "stress = [\"In today's economic upheavals, downsizing, layoff, merger and bankruptcies have cost hundreds of thousands of workers their jobs. Millions more have been shifted to unfamiliar tasks within their companies and wonder how much longer they will be employed. Adding to the pressures that workers face are new bosses, computer surveillance of production, fewer health and retirement benefits, and the\",\n",
    "          \"feeling they have to work longer and harder just to maintain their current economic status. Workers at every level are experiencing increased tension and uncertainty, and are updating their resumes. The loss of a job can be devastating, putting unemployed workers at risk for physical illness, marital strain, anxiety, depression and even suicide.\",\n",
    "          \" Loss of a job affects every part of life, from what time you get up in the morning, to whom you see and what you can afford to do. Until the transition is made to a new position, stress is chronic.\",\n",
    "          \"A feeling of powerlessness is a universal cause of job stress. When you feel powerless, you're prey to depression's traveling companions, helplessness and hopelessness. You don't alter or avoid the situation because you feel nothing can be done.\",\n",
    "          \"Secretaries, waitresses, middle managers, police officers, editors and medical interns are among those with the most highly stressed occupations marked by the need to respond to others' demands and timetables, with little control over events. Common to this job situation are complaints of too much responsibility and too little authority, unfair labor practices and inadequate job descriptions.\",\n",
    "          \" Employees can counteract these pressures through workers' unions or other organizations, grievance or personnel offices or, more commonly, by direct negotiations with their immediate supervisors.\",\n",
    "          \"Every employee should have a specific, written job description. Simply negotiating one does more to dispel a sense of powerlessness than anything else we know. It is a contract that you help write. You can object to what and insist on what you do want. If there is a compromise, it's because you agreed to it. With a clear job description, your expectations are spelled out, as are your boss's.\",\n",
    "          \"A good job description is time limited. Set a specific date for a review and revision based on your mutual experience with this initial job description. If you and your boss can't agree on what your job description should be, look for another job, either within the same company or outside. Even in these tough economic times, it is important that your job be a source of satisfaction and respect.\"]\n",
    "\n",
    "\n",
    "sentences = [word_tokenize(text) for text in stress]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'todays',\n",
       "  'economic',\n",
       "  'upheavals',\n",
       "  'downsizing',\n",
       "  'layoff',\n",
       "  'merger',\n",
       "  'and',\n",
       "  'bankruptcies',\n",
       "  'have',\n",
       "  'cost',\n",
       "  'hundreds',\n",
       "  'of',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'workers',\n",
       "  'their',\n",
       "  'jobs',\n",
       "  'Millions',\n",
       "  'more',\n",
       "  'have',\n",
       "  'been',\n",
       "  'shifted',\n",
       "  'to',\n",
       "  'unfamiliar',\n",
       "  'tasks',\n",
       "  'within',\n",
       "  'their',\n",
       "  'companies',\n",
       "  'and',\n",
       "  'wonder',\n",
       "  'how',\n",
       "  'much',\n",
       "  'longer',\n",
       "  'they',\n",
       "  'will',\n",
       "  'be',\n",
       "  'employed',\n",
       "  'Adding',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pressures',\n",
       "  'that',\n",
       "  'workers',\n",
       "  'face',\n",
       "  'are',\n",
       "  'new',\n",
       "  'bosses',\n",
       "  'computer',\n",
       "  'surveillance',\n",
       "  'of',\n",
       "  'production',\n",
       "  'fewer',\n",
       "  'health',\n",
       "  'and',\n",
       "  'retirement',\n",
       "  'benefits',\n",
       "  'and',\n",
       "  'the'],\n",
       " ['feeling',\n",
       "  'they',\n",
       "  'have',\n",
       "  'to',\n",
       "  'work',\n",
       "  'longer',\n",
       "  'and',\n",
       "  'harder',\n",
       "  'just',\n",
       "  'to',\n",
       "  'maintain',\n",
       "  'their',\n",
       "  'current',\n",
       "  'economic',\n",
       "  'status',\n",
       "  'Workers',\n",
       "  'at',\n",
       "  'every',\n",
       "  'level',\n",
       "  'are',\n",
       "  'experiencing',\n",
       "  'increased',\n",
       "  'tension',\n",
       "  'and',\n",
       "  'uncertainty',\n",
       "  'and',\n",
       "  'are',\n",
       "  'updating',\n",
       "  'their',\n",
       "  'resumes',\n",
       "  'The',\n",
       "  'loss',\n",
       "  'of',\n",
       "  'a',\n",
       "  'job',\n",
       "  'can',\n",
       "  'be',\n",
       "  'devastating',\n",
       "  'putting',\n",
       "  'unemployed',\n",
       "  'workers',\n",
       "  'at',\n",
       "  'risk',\n",
       "  'for',\n",
       "  'physical',\n",
       "  'illness',\n",
       "  'marital',\n",
       "  'strain',\n",
       "  'anxiety',\n",
       "  'depression',\n",
       "  'and',\n",
       "  'even',\n",
       "  'suicide'],\n",
       " ['Loss',\n",
       "  'of',\n",
       "  'a',\n",
       "  'job',\n",
       "  'affects',\n",
       "  'every',\n",
       "  'part',\n",
       "  'of',\n",
       "  'life',\n",
       "  'from',\n",
       "  'what',\n",
       "  'time',\n",
       "  'you',\n",
       "  'get',\n",
       "  'up',\n",
       "  'in',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'to',\n",
       "  'whom',\n",
       "  'you',\n",
       "  'see',\n",
       "  'and',\n",
       "  'what',\n",
       "  'you',\n",
       "  'can',\n",
       "  'afford',\n",
       "  'to',\n",
       "  'do',\n",
       "  'Until',\n",
       "  'the',\n",
       "  'transition',\n",
       "  'is',\n",
       "  'made',\n",
       "  'to',\n",
       "  'a',\n",
       "  'new',\n",
       "  'position',\n",
       "  'stress',\n",
       "  'is',\n",
       "  'chronic'],\n",
       " ['A',\n",
       "  'feeling',\n",
       "  'of',\n",
       "  'powerlessness',\n",
       "  'is',\n",
       "  'a',\n",
       "  'universal',\n",
       "  'cause',\n",
       "  'of',\n",
       "  'job',\n",
       "  'stress',\n",
       "  'When',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'powerless',\n",
       "  'youre',\n",
       "  'prey',\n",
       "  'to',\n",
       "  'depressions',\n",
       "  'traveling',\n",
       "  'companions',\n",
       "  'helplessness',\n",
       "  'and',\n",
       "  'hopelessness',\n",
       "  'You',\n",
       "  'dont',\n",
       "  'alter',\n",
       "  'or',\n",
       "  'avoid',\n",
       "  'the',\n",
       "  'situation',\n",
       "  'because',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'nothing',\n",
       "  'can',\n",
       "  'be',\n",
       "  'done'],\n",
       " ['Secretaries',\n",
       "  'waitresses',\n",
       "  'middle',\n",
       "  'managers',\n",
       "  'police',\n",
       "  'officers',\n",
       "  'editors',\n",
       "  'and',\n",
       "  'medical',\n",
       "  'interns',\n",
       "  'are',\n",
       "  'among',\n",
       "  'those',\n",
       "  'with',\n",
       "  'the',\n",
       "  'most',\n",
       "  'highly',\n",
       "  'stressed',\n",
       "  'occupations',\n",
       "  'marked',\n",
       "  'by',\n",
       "  'the',\n",
       "  'need',\n",
       "  'to',\n",
       "  'respond',\n",
       "  'to',\n",
       "  'others',\n",
       "  'demands',\n",
       "  'and',\n",
       "  'timetables',\n",
       "  'with',\n",
       "  'little',\n",
       "  'control',\n",
       "  'over',\n",
       "  'events',\n",
       "  'Common',\n",
       "  'to',\n",
       "  'this',\n",
       "  'job',\n",
       "  'situation',\n",
       "  'are',\n",
       "  'complaints',\n",
       "  'of',\n",
       "  'too',\n",
       "  'much',\n",
       "  'responsibility',\n",
       "  'and',\n",
       "  'too',\n",
       "  'little',\n",
       "  'authority',\n",
       "  'unfair',\n",
       "  'labor',\n",
       "  'practices',\n",
       "  'and',\n",
       "  'inadequate',\n",
       "  'job',\n",
       "  'descriptions'],\n",
       " ['Employees',\n",
       "  'can',\n",
       "  'counteract',\n",
       "  'these',\n",
       "  'pressures',\n",
       "  'through',\n",
       "  'workers',\n",
       "  'unions',\n",
       "  'or',\n",
       "  'other',\n",
       "  'organizations',\n",
       "  'grievance',\n",
       "  'or',\n",
       "  'personnel',\n",
       "  'offices',\n",
       "  'or',\n",
       "  'more',\n",
       "  'commonly',\n",
       "  'by',\n",
       "  'direct',\n",
       "  'negotiations',\n",
       "  'with',\n",
       "  'their',\n",
       "  'immediate',\n",
       "  'supervisors'],\n",
       " ['Every',\n",
       "  'employee',\n",
       "  'should',\n",
       "  'have',\n",
       "  'a',\n",
       "  'specific',\n",
       "  'written',\n",
       "  'job',\n",
       "  'description',\n",
       "  'Simply',\n",
       "  'negotiating',\n",
       "  'one',\n",
       "  'does',\n",
       "  'more',\n",
       "  'to',\n",
       "  'dispel',\n",
       "  'a',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'powerlessness',\n",
       "  'than',\n",
       "  'anything',\n",
       "  'else',\n",
       "  'we',\n",
       "  'know',\n",
       "  'It',\n",
       "  'is',\n",
       "  'a',\n",
       "  'contract',\n",
       "  'that',\n",
       "  'you',\n",
       "  'help',\n",
       "  'write',\n",
       "  'You',\n",
       "  'can',\n",
       "  'object',\n",
       "  'to',\n",
       "  'what',\n",
       "  'and',\n",
       "  'insist',\n",
       "  'on',\n",
       "  'what',\n",
       "  'you',\n",
       "  'do',\n",
       "  'want',\n",
       "  'If',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'compromise',\n",
       "  'its',\n",
       "  'because',\n",
       "  'you',\n",
       "  'agreed',\n",
       "  'to',\n",
       "  'it',\n",
       "  'With',\n",
       "  'a',\n",
       "  'clear',\n",
       "  'job',\n",
       "  'description',\n",
       "  'your',\n",
       "  'expectations',\n",
       "  'are',\n",
       "  'spelled',\n",
       "  'out',\n",
       "  'as',\n",
       "  'are',\n",
       "  'your',\n",
       "  'bosss'],\n",
       " ['A',\n",
       "  'good',\n",
       "  'job',\n",
       "  'description',\n",
       "  'is',\n",
       "  'time',\n",
       "  'limited',\n",
       "  'Set',\n",
       "  'a',\n",
       "  'specific',\n",
       "  'date',\n",
       "  'for',\n",
       "  'a',\n",
       "  'review',\n",
       "  'and',\n",
       "  'revision',\n",
       "  'based',\n",
       "  'on',\n",
       "  'your',\n",
       "  'mutual',\n",
       "  'experience',\n",
       "  'with',\n",
       "  'this',\n",
       "  'initial',\n",
       "  'job',\n",
       "  'description',\n",
       "  'If',\n",
       "  'you',\n",
       "  'and',\n",
       "  'your',\n",
       "  'boss',\n",
       "  'cant',\n",
       "  'agree',\n",
       "  'on',\n",
       "  'what',\n",
       "  'your',\n",
       "  'job',\n",
       "  'description',\n",
       "  'should',\n",
       "  'be',\n",
       "  'look',\n",
       "  'for',\n",
       "  'another',\n",
       "  'job',\n",
       "  'either',\n",
       "  'within',\n",
       "  'the',\n",
       "  'same',\n",
       "  'company',\n",
       "  'or',\n",
       "  'outside',\n",
       "  'Even',\n",
       "  'in',\n",
       "  'these',\n",
       "  'tough',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'it',\n",
       "  'is',\n",
       "  'important',\n",
       "  'that',\n",
       "  'your',\n",
       "  'job',\n",
       "  'be',\n",
       "  'a',\n",
       "  'source',\n",
       "  'of',\n",
       "  'satisfaction',\n",
       "  'and',\n",
       "  'respect']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_jobs(jobs):\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    jobs = [j.translate(table) for j in jobs]\n",
    "    # tokenize words\n",
    "    return [word_tokenize(j) for j in jobs]\n",
    "\n",
    "sentences = tokenize_jobs(stress)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "help(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_check_input_data_sanity',\n",
       " '_check_training_sanity',\n",
       " '_clear_post_train',\n",
       " '_do_train_epoch',\n",
       " '_do_train_job',\n",
       " '_get_job_params',\n",
       " '_get_thread_working_mem',\n",
       " '_job_producer',\n",
       " '_load_specials',\n",
       " '_log_epoch_end',\n",
       " '_log_epoch_progress',\n",
       " '_log_progress',\n",
       " '_log_train_end',\n",
       " '_minimize_model',\n",
       " '_raw_word_count',\n",
       " '_save_specials',\n",
       " '_set_train_params',\n",
       " '_smart_save',\n",
       " '_train_epoch',\n",
       " '_train_epoch_corpusfile',\n",
       " '_update_job_params',\n",
       " '_worker_loop',\n",
       " '_worker_loop_corpusfile',\n",
       " 'accuracy',\n",
       " 'alpha',\n",
       " 'batch_words',\n",
       " 'build_vocab',\n",
       " 'build_vocab_from_freq',\n",
       " 'callbacks',\n",
       " 'cbow_mean',\n",
       " 'clear_sims',\n",
       " 'compute_loss',\n",
       " 'corpus_count',\n",
       " 'corpus_total_words',\n",
       " 'cum_table',\n",
       " 'delete_temporary_training_data',\n",
       " 'doesnt_match',\n",
       " 'epochs',\n",
       " 'estimate_memory',\n",
       " 'evaluate_word_pairs',\n",
       " 'get_latest_training_loss',\n",
       " 'hashfxn',\n",
       " 'hs',\n",
       " 'init_sims',\n",
       " 'intersect_word2vec_format',\n",
       " 'iter',\n",
       " 'layer1_size',\n",
       " 'load',\n",
       " 'load_word2vec_format',\n",
       " 'log_accuracy',\n",
       " 'max_final_vocab',\n",
       " 'min_alpha',\n",
       " 'min_alpha_yet_reached',\n",
       " 'min_count',\n",
       " 'model_trimmed_post_training',\n",
       " 'most_similar',\n",
       " 'most_similar_cosmul',\n",
       " 'n_similarity',\n",
       " 'negative',\n",
       " 'ns_exponent',\n",
       " 'predict_output_word',\n",
       " 'random',\n",
       " 'reset_from',\n",
       " 'running_training_loss',\n",
       " 'sample',\n",
       " 'save',\n",
       " 'save_word2vec_format',\n",
       " 'score',\n",
       " 'sg',\n",
       " 'similar_by_vector',\n",
       " 'similar_by_word',\n",
       " 'similarity',\n",
       " 'syn0_lockf',\n",
       " 'syn1',\n",
       " 'syn1neg',\n",
       " 'total_train_time',\n",
       " 'train',\n",
       " 'train_count',\n",
       " 'trainables',\n",
       " 'vector_size',\n",
       " 'vocabulary',\n",
       " 'window',\n",
       " 'wmdistance',\n",
       " 'workers',\n",
       " 'wv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences, min_count=1, size=5)\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=248, size=5, alpha=0.025)\n",
      "['In', 'todays', 'economic', 'upheavals', 'downsizing', 'layoff', 'merger', 'and', 'bankruptcies', 'have', 'cost', 'hundreds', 'of', 'thousands', 'workers', 'their', 'jobs', 'Millions', 'more', 'been', 'shifted', 'to', 'unfamiliar', 'tasks', 'within', 'companies', 'wonder', 'how', 'much', 'longer', 'they', 'will', 'be', 'employed', 'Adding', 'the', 'pressures', 'that', 'face', 'are', 'new', 'bosses', 'computer', 'surveillance', 'production', 'fewer', 'health', 'retirement', 'benefits', 'feeling', 'work', 'harder', 'just', 'maintain', 'current', 'status', 'Workers', 'at', 'every', 'level', 'experiencing', 'increased', 'tension', 'uncertainty', 'updating', 'resumes', 'The', 'loss', 'a', 'job', 'can', 'devastating', 'putting', 'unemployed', 'risk', 'for', 'physical', 'illness', 'marital', 'strain', 'anxiety', 'depression', 'even', 'suicide', 'Loss', 'affects', 'part', 'life', 'from', 'what', 'time', 'you', 'get', 'up', 'in', 'morning', 'whom', 'see', 'afford', 'do', 'Until', 'transition', 'is', 'made', 'position', 'stress', 'chronic', 'A', 'powerlessness', 'universal', 'cause', 'When', 'feel', 'powerless', 'youre', 'prey', 'depressions', 'traveling', 'companions', 'helplessness', 'hopelessness', 'You', 'dont', 'alter', 'or', 'avoid', 'situation', 'because', 'nothing', 'done', 'Secretaries', 'waitresses', 'middle', 'managers', 'police', 'officers', 'editors', 'medical', 'interns', 'among', 'those', 'with', 'most', 'highly', 'stressed', 'occupations', 'marked', 'by', 'need', 'respond', 'others', 'demands', 'timetables', 'little', 'control', 'over', 'events', 'Common', 'this', 'complaints', 'too', 'responsibility', 'authority', 'unfair', 'labor', 'practices', 'inadequate', 'descriptions', 'Employees', 'counteract', 'these', 'through', 'unions', 'other', 'organizations', 'grievance', 'personnel', 'offices', 'commonly', 'direct', 'negotiations', 'immediate', 'supervisors', 'Every', 'employee', 'should', 'specific', 'written', 'description', 'Simply', 'negotiating', 'one', 'does', 'dispel', 'sense', 'than', 'anything', 'else', 'we', 'know', 'It', 'contract', 'help', 'write', 'object', 'insist', 'on', 'want', 'If', 'there', 'compromise', 'its', 'agreed', 'it', 'With', 'clear', 'your', 'expectations', 'spelled', 'out', 'as', 'bosss', 'good', 'limited', 'Set', 'date', 'review', 'revision', 'based', 'mutual', 'experience', 'initial', 'boss', 'cant', 'agree', 'look', 'another', 'either', 'same', 'company', 'outside', 'Even', 'tough', 'times', 'important', 'source', 'satisfaction', 'respect']\n",
      "248\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(list(model.wv.vocab))\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08808291 -0.04018605 -0.01458764 -0.01262254 -0.09677621]\n",
      " [ 0.08538022 -0.01452671  0.05317906  0.00839999  0.0089781 ]\n",
      " [ 0.05581348 -0.02680791 -0.06789459  0.08080404 -0.09858041]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 Output vectors for specific tokens\n",
    "print(model.wv['Secretaries', 'resumes', 'pressures'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dont', 0.9333158731460571),\n",
       " ('at', 0.9168218374252319),\n",
       " ('cant', 0.9026200771331787),\n",
       " ('direct', 0.8324217796325684),\n",
       " ('counteract', 0.8054370880126953),\n",
       " ('Every', 0.7731367349624634),\n",
       " ('maintain', 0.7565508484840393),\n",
       " ('benefits', 0.748710572719574),\n",
       " ('Secretaries', 0.7460527420043945),\n",
       " ('The', 0.741858959197998)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('Employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPtvIrZQ4t_m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Importing more data to work with\n",
    "text_from_corpus = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4111
    },
    "colab_type": "code",
    "id": "_DFfIq1M3GBg",
    "outputId": "f3776d7f-f89f-4f55-b74b-086c70df2932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6 Investigate what we just downloaded\n",
    "print(dir(text_from_corpus))\n",
    "print(text_from_corpus.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'lerxstwamumdedu', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntppostinghost', 'rac3wamumdedu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was', 'a', '2door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'email', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst'], ['from', 'guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelley1qvfo9innc3s', 'organization', 'university', 'of', 'washington', 'lines', '11', 'nntppostinghost', 'carsonuwashingtonedu', 'a', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '14', 'm', 'floppies', 'are', 'especially', 'requested', 'i', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'havent', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu'], ['from', 'twillisececnpurdueedu', 'thomas', 'e', 'willis', 'subject', 'pb', 'questions', 'organization', 'purdue', 'university', 'engineering', 'computer', 'network', 'distribution', 'usa', 'lines', '36', 'well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back', 'in', '1985', 'sooo', 'im', 'in', 'the', 'market', 'for', 'a', 'new', 'machine', 'a', 'bit', 'sooner', 'than', 'i', 'intended', 'to', 'be', 'im', 'looking', 'into', 'picking', 'up', 'a', 'powerbook', '160', 'or', 'maybe', '180', 'and', 'have', 'a', 'bunch', 'of', 'questions', 'that', 'hopefully', 'somebody', 'can', 'answer', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook', 'introductions', 'are', 'expected', 'id', 'heard', 'the', '185c', 'was', 'supposed', 'to', 'make', 'an', 'appearence', 'this', 'summer', 'but', 'havent', 'heard', 'anymore', 'on', 'it', 'and', 'since', 'i', 'dont', 'have', 'access', 'to', 'macleak', 'i', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had', 'more', 'info', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the', 'ones', 'the', 'duos', 'just', 'went', 'through', 'recently', 'whats', 'the', 'impression', 'of', 'the', 'display', 'on', 'the', '180', 'i', 'could', 'probably', 'swing', 'a', '180', 'if', 'i', 'got', 'the', '80mb', 'disk', 'rather', 'than', 'the', '120', 'but', 'i', 'dont', 'really', 'have', 'a', 'feel', 'for', 'how', 'much', 'better', 'the', 'display', 'is', 'yea', 'it', 'looks', 'great', 'in', 'the', 'store', 'but', 'is', 'that', 'all', 'wow', 'or', 'is', 'it', 'really', 'that', 'good', 'could', 'i', 'solicit', 'some', 'opinions', 'of', 'people', 'who', 'use', 'the', '160', 'and', '180', 'daytoday', 'on', 'if', 'its', 'worth', 'taking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', 'i', 'realize', 'this', 'is', 'a', 'real', 'subjective', 'question', 'but', 'ive', 'only', 'played', 'around', 'with', 'the', 'machines', 'in', 'a', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody', 'who', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', 'how', 'well', 'does', 'hellcats', 'perform', 'thanks', 'a', 'bunch', 'in', 'advance', 'for', 'any', 'info', 'if', 'you', 'could', 'email', 'ill', 'post', 'a', 'summary', 'news', 'reading', 'time', 'is', 'at', 'a', 'premium', 'with', 'finals', 'just', 'around', 'the', 'corner', 'tom', 'willis', 'twillisecnpurdueedu', 'purdue', 'electrical', 'engineering', 'convictions', 'are', 'more', 'dangerous', 'enemies', 'of', 'truth', 'than', 'lies', 'f', 'w', 'nietzsche'], ['from', 'jgreenamber', 'joe', 'green', 'subject', 're', 'weitek', 'p9000', 'organization', 'harris', 'computer', 'systems', 'division', 'lines', '14', 'distribution', 'world', 'nntppostinghost', 'amberssdcsdharriscom', 'xnewsreader', 'tin', 'version', '11', 'pl9', 'robert', 'jc', 'kyanko', 'robrjckuucp', 'wrote', 'abraxisiastateedu', 'writes', 'in', 'article', 'abraxis734340159class1iastateedu', 'anyone', 'know', 'about', 'the', 'weitek', 'p9000', 'graphics', 'chip', 'as', 'far', 'as', 'the', 'lowlevel', 'stuff', 'goes', 'it', 'looks', 'pretty', 'nice', 'its', 'got', 'this', 'quadrilateral', 'fill', 'command', 'that', 'requires', 'just', 'the', 'four', 'points', 'do', 'you', 'have', 'weiteks', 'addressphone', 'number', 'id', 'like', 'to', 'get', 'some', 'information', 'about', 'this', 'chip', 'joe', 'green', 'harris', 'corporation', 'jgreencsdharriscom', 'computer', 'systems', 'division', 'the', 'only', 'thing', 'that', 'really', 'scares', 'me', 'is', 'a', 'person', 'with', 'no', 'sense', 'of', 'humor', 'jonathan', 'winters'], ['from', 'jcmheadcfaharvardedu', 'jonathan', 'mcdowell', 'subject', 're', 'shuttle', 'launch', 'question', 'organization', 'smithsonian', 'astrophysical', 'observatory', 'cambridge', 'ma', 'usa', 'distribution', 'sci', 'lines', '23', 'from', 'article', 'c5owcbn3pworldstdcom', 'by', 'tombakerworldstdcom', 'tom', 'a', 'baker', 'in', 'article', 'c5jlwx4h91cscmuedu', 'etratttacs1ttuedu', 'pack', 'rat', 'writes', 'clear', 'caution', 'warning', 'memory', 'verify', 'no', 'unexpected', 'errors', 'i', 'am', 'wondering', 'what', 'an', 'expected', 'error', 'might', 'be', 'sorry', 'if', 'this', 'is', 'a', 'really', 'dumb', 'question', 'but', 'parity', 'errors', 'in', 'memory', 'or', 'previously', 'known', 'conditions', 'that', 'were', 'waivered', 'yes', 'that', 'is', 'an', 'error', 'but', 'we', 'already', 'knew', 'about', 'it', 'id', 'be', 'curious', 'as', 'to', 'what', 'the', 'real', 'meaning', 'of', 'the', 'quote', 'is', 'tom', 'my', 'understanding', 'is', 'that', 'the', 'expected', 'errors', 'are', 'basically', 'known', 'bugs', 'in', 'the', 'warning', 'system', 'software', 'things', 'are', 'checked', 'that', 'dont', 'have', 'the', 'right', 'values', 'in', 'yet', 'because', 'they', 'arent', 'set', 'till', 'after', 'launch', 'and', 'suchlike', 'rather', 'than', 'fix', 'the', 'code', 'and', 'possibly', 'introduce', 'new', 'bugs', 'they', 'just', 'tell', 'the', 'crew', 'ok', 'if', 'you', 'see', 'a', 'warning', 'no', '213', 'before', 'liftoff', 'ignore', 'it', 'jonathan'], ['from', 'dfovttoulutkovttfi', 'foxvog', 'douglas', 'subject', 're', 'rewording', 'the', 'second', 'amendment', 'ideas', 'organization', 'vtt', 'lines', '58', 'in', 'article', '1r1eu14ttransferstratuscom', 'cdtswstratuscom', 'c', 'd', 'tavares', 'writes', 'in', 'article', '1993apr2008305716899ousrvroulufi', 'dfovttoulutkovttfi', 'foxvog', 'douglas', 'writes', 'in', 'article', '1qv87v4j3transferstratuscom', 'cdtswstratuscom', 'c', 'd', 'tavares', 'writes', 'in', 'article', 'c5n3gif8fulowellulowelledu', 'jrutledgcsulowelledu', 'john', 'lawrence', 'rutledge', 'writes', 'the', 'massive', 'destructive', 'power', 'of', 'many', 'modern', 'weapons', 'makes', 'the', 'cost', 'of', 'an', 'accidental', 'or', 'crimial', 'usage', 'of', 'these', 'weapons', 'to', 'great', 'the', 'weapons', 'of', 'mass', 'destruction', 'need', 'to', 'be', 'in', 'the', 'control', 'of', 'the', 'government', 'only', 'individual', 'access', 'would', 'result', 'in', 'the', 'needless', 'deaths', 'of', 'millions', 'this', 'makes', 'the', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear', 'many', 'modern', 'weapons', 'nonexistant', 'thanks', 'for', 'stating', 'where', 'youre', 'coming', 'from', 'needless', 'to', 'say', 'i', 'disagree', 'on', 'every', 'count', 'you', 'believe', 'that', 'individuals', 'should', 'have', 'the', 'right', 'to', 'own', 'weapons', 'of', 'mass', 'destruction', 'i', 'find', 'it', 'hard', 'to', 'believe', 'that', 'you', 'would', 'support', 'a', 'neighbors', 'right', 'to', 'keep', 'nuclear', 'weapons', 'biological', 'weapons', 'and', 'nerve', 'gas', 'on', 'hisher', 'property', 'if', 'we', 'can', 'not', 'even', 'agree', 'on', 'keeping', 'weapons', 'of', 'mass', 'destruction', 'out', 'of', 'the', 'hands', 'of', 'individuals', 'can', 'there', 'be', 'any', 'hope', 'for', 'us', 'i', 'dont', 'sign', 'any', 'blank', 'checks', 'of', 'course', 'the', 'term', 'must', 'be', 'rigidly', 'defined', 'in', 'any', 'bill', 'when', 'doug', 'foxvog', 'says', 'weapons', 'of', 'mass', 'destruction', 'he', 'means', 'cbw', 'and', 'nukes', 'when', 'sarah', 'brady', 'says', 'weapons', 'of', 'mass', 'destruction', 'she', 'means', 'street', 'sweeper', 'shotguns', 'and', 'semiautomatic', 'sks', 'rifles', 'i', 'doubt', 'she', 'uses', 'this', 'term', 'for', 'that', 'you', 'are', 'using', 'a', 'quote', 'allegedly', 'from', 'her', 'can', 'you', 'back', 'it', 'up', 'when', 'john', 'lawrence', 'rutledge', 'says', 'weapons', 'of', 'mass', 'destruction', 'and', 'then', 'immediately', 'follows', 'it', 'with', 'the', 'us', 'has', 'thousands', 'of', 'people', 'killed', 'each', 'year', 'by', 'handguns', 'this', 'number', 'can', 'easily', 'be', 'reduced', 'by', 'putting', 'reasonable', 'restrictions', 'on', 'them', 'what', 'does', 'rutledge', 'mean', 'by', 'the', 'term', 'i', 'read', 'the', 'article', 'as', 'presenting', 'first', 'an', 'argument', 'about', 'weapons', 'of', 'mass', 'destruction', 'as', 'commonly', 'understood', 'and', 'then', 'switching', 'to', 'other', 'topics', 'the', 'first', 'point', 'evidently', 'was', 'to', 'show', 'that', 'not', 'all', 'weapons', 'should', 'be', 'allowed', 'and', 'then', 'the', 'later', 'analysis', 'was', 'given', 'this', 'understanding', 'to', 'consider', 'another', 'class', 'cdtrocketswstratuscom', 'if', 'you', 'believe', 'that', 'i', 'speak', 'for', 'my', 'company', 'or', 'cdtvosstratuscom', 'write', 'today', 'for', 'my', 'special', 'investors', 'packet', 'doug', 'foxvog', 'douglasfoxvogvttfi'], ['from', 'bmdelanequadsuchicagoedu', 'brian', 'manning', 'delaney', 'subject', 'brain', 'tumor', 'treatment', 'thanks', 'replyto', 'bmdelanemidwayuchicagoedu', 'organization', 'university', 'of', 'chicago', 'lines', '12', 'there', 'were', 'a', 'few', 'people', 'who', 'responded', 'to', 'my', 'request', 'for', 'info', 'on', 'treatment', 'for', 'astrocytomas', 'through', 'email', 'whom', 'i', 'couldnt', 'thank', 'directly', 'because', 'of', 'mailbouncing', 'probs', 'sean', 'debra', 'and', 'sharon', 'so', 'i', 'thought', 'id', 'publicly', 'thank', 'everyone', 'thanks', 'im', 'sure', 'glad', 'i', 'accidentally', 'hit', 'rn', 'instead', 'of', 'rm', 'when', 'i', 'was', 'trying', 'to', 'delete', 'a', 'file', 'last', 'september', 'hmmm', 'news', 'whats', 'this', 'brian'], ['from', 'bgrubbdantenmsuedu', 'grubb', 'subject', 're', 'ide', 'vs', 'scsi', 'organization', 'new', 'mexico', 'state', 'university', 'las', 'cruces', 'nm', 'lines', '44', 'distribution', 'world', 'nntppostinghost', 'dantenmsuedu', 'dxb132psuvmpsuedu', 'writes', 'in', 'article', '1qlbrlinn7rkdns1nmsuedu', 'bgrubbdantenmsuedu', 'grubb', 'says', 'in', 'pc', 'magazine', 'april', '27', '199329', 'although', 'scsi', 'is', 'twice', 'as', 'fasst', 'as', 'esdi', '20', 'faster', 'than', 'ide', 'and', 'support', 'up', 'to', '7', 'devices', 'its', 'acceptance', 'has', 'long', 'been', 'stalled', 'by', 'incompatability', 'problems', 'and', 'installation', 'headaches', 'i', 'love', 'it', 'when', 'magazine', 'writers', 'make', 'stupid', 'statements', 'like', 'that', 're', 'performance', 'where', 'do', 'they', 'get', 'those', 'numbers', 'ill', 'list', 'the', 'actual', 'performance', 'ranges', 'which', 'should', 'convince', 'anyone', 'that', 'such', 'a', 'statement', 'is', 'absurd', 'scsii', 'ranges', 'from', '05mbs', 'scsiii', 'ranges', 'from', '040mbs', 'ide', 'ranges', 'from', '083mbs', 'esdi', 'is', 'always', '125mbs', 'although', 'there', 'are', 'some', 'nonstandard', 'versions', 'all', 'this', 'shows', 'is', 'that', 'you', 'dont', 'know', 'much', 'about', 'scsi', 'scsi1', 'with', 'a', 'scsi1', 'controler', 'chip', 'range', 'is', 'indeed', '05mbs', 'and', 'that', 'is', 'all', 'you', 'have', 'right', 'about', 'scsi', 'scsi1', 'with', 'a', 'scsi2', 'controller', 'chip', '46mbs', 'with', '10mbs', 'burst', '8bit', 'note', 'the', 'increase', 'in', 'speed', 'the', 'mac', 'quadra', 'uses', 'this', 'version', 'of', 'scsi1', 'so', 'it', 'does', 'exist', 'some', 'pc', 'use', 'this', 'set', 'up', 'too', 'scsi2', '8bitscsi1', 'mode', '46mbs', 'with', '10mbs', 'burst', 'scsi2', '16bitwide', 'or', 'fast', 'mode', '812mbs', 'with', '20mbs', 'burst', 'scsi2', '32bitwide', 'and', 'fast', '1520mbs', 'with', '40mbs', 'burst', 'by', 'your', 'own', 'data', 'the', 'although', 'scsi', 'is', 'twice', 'as', 'fast', 'as', 'esdi', 'is', 'correct', 'with', 'a', 'scsi2', 'controller', 'chip', 'scsi1', 'can', 'reach', '10mbs', 'which', 'is', 'indeed', '20', 'faster', 'than', 'ide', '120', 'of', '83', 'is', '996', 'all', 'these', 'scsi', 'facts', 'have', 'been', 'posted', 'to', 'this', 'newsgroup', 'in', 'my', 'mac', 'ibm', 'info', 'sheet', 'available', 'by', 'ftp', 'on', 'sumexaimstanfordedu', '364406', 'in', 'the', 'infomacreport', 'as', 'macibmcompareversion', 'txt', 'it', 'should', 'be', '173', 'but', '161', 'may', 'still', 'be', 'there', 'part', 'of', 'this', 'problem', 'is', 'both', 'mac', 'and', 'ibm', 'pc', 'are', 'inconsiant', 'about', 'what', 'scsi', 'is', 'which', 'though', 'it', 'is', 'well', 'documented', 'that', 'the', 'quadra', 'has', 'a', 'scsi2', 'chip', 'an', 'apple', 'salesperson', 'said', 'it', 'uses', 'a', 'fast', 'scsi1', 'chip', 'not', 'at', 'a', '6mbs', '10mbs', 'burst', 'it', 'does', 'not', 'scsi1', 'is', '5mbs', 'maximum', 'synchronous', 'and', 'quadra', 'uses', 'ansynchronous', 'scsi', 'which', 'is', 'slower', 'it', 'seems', 'that', 'mac', 'and', 'ibm', 'see', 'scsi1', 'interface', 'and', 'think', 'scsi1', 'when', 'it', 'maybe', 'a', 'scsi1', 'interface', 'driven', 'in', 'the', 'machine', 'by', 'a', 'scsi2', 'controller', 'chip', 'in', '8bit', 'mode', 'which', 'is', 'much', 'faster', 'then', 'true', 'scsi1', 'can', 'go', 'dont', 'slam', 'an', 'article', 'because', 'you', 'dont', 'understand', 'what', 'is', 'going', 'on', 'one', 'reference', 'for', 'the', 'quadras', 'scsi2', 'controller', 'chip', 'is', 'digital', 'review', 'oct', '21', '1991', 'v8', 'n33', 'p81'], ['from', 'holmes7000iscsvaxuniedu', 'subject', 'win', '30', 'icon', 'help', 'please', 'organization', 'university', 'of', 'northern', 'iowa', 'lines', '10', 'i', 'have', 'win', '30', 'and', 'downloaded', 'several', 'icons', 'and', 'bmps', 'but', 'i', 'cant', 'figure', 'out', 'how', 'to', 'change', 'the', 'wallpaper', 'or', 'use', 'the', 'icons', 'any', 'help', 'would', 'be', 'appreciated', 'thanx', 'brando', 'ps', 'please', 'email', 'me'], ['from', 'kerrux1csouiucedu', 'stan', 'kerr', 'subject', 're', 'sigma', 'designs', 'double', 'up', 'articleid', 'ux1c52u8xb62', 'organization', 'university', 'of', 'illinois', 'at', 'urbana', 'lines', '29', 'jap10pocwruedu', 'joseph', 'a', 'pellettiere', 'writes', 'i', 'am', 'looking', 'for', 'any', 'information', 'about', 'the', 'sigma', 'designs', 'double', 'up', 'board', 'all', 'i', 'can', 'figure', 'out', 'is', 'that', 'it', 'is', 'a', 'hardware', 'compression', 'board', 'that', 'works', 'with', 'autodoubler', 'but', 'i', 'am', 'not', 'sure', 'about', 'this', 'also', 'how', 'much', 'would', 'one', 'cost', 'ive', 'had', 'the', 'board', 'for', 'over', 'a', 'year', 'and', 'it', 'does', 'work', 'with', 'diskdoubler', 'but', 'not', 'with', 'autodoubler', 'due', 'to', 'a', 'licensing', 'problem', 'with', 'stac', 'technologies', 'the', 'owners', 'of', 'the', 'boards', 'compression', 'technology', 'im', 'writing', 'this', 'from', 'memory', 'ive', 'lost', 'the', 'reference', 'please', 'correct', 'me', 'if', 'im', 'wrong', 'using', 'the', 'board', 'ive', 'had', 'problems', 'with', 'file', 'icons', 'being', 'lost', 'but', 'its', 'hard', 'to', 'say', 'whether', 'its', 'the', 'boards', 'fault', 'or', 'something', 'else', 'however', 'if', 'i', 'decompress', 'the', 'troubled', 'file', 'and', 'recompress', 'it', 'without', 'the', 'board', 'the', 'icon', 'usually', 'reappears', 'because', 'of', 'the', 'above', 'mentioned', 'licensing', 'problem', 'the', 'freeware', 'expansion', 'utility', 'dd', 'expand', 'will', 'not', 'decompress', 'a', 'boardcompressed', 'file', 'unless', 'you', 'have', 'the', 'board', 'installed', 'since', 'stac', 'has', 'its', 'own', 'product', 'now', 'it', 'seems', 'unlikely', 'that', 'the', 'holes', 'in', 'autodoublerdiskdoubler', 'related', 'to', 'the', 'board', 'will', 'be', 'fixed', 'which', 'is', 'sad', 'and', 'makes', 'me', 'very', 'reluctant', 'to', 'buy', 'stacs', 'product', 'since', 'theyre', 'being', 'so', 'stinky', 'but', 'hey', 'thats', 'competition', 'stan', 'kerr', 'computing', 'communications', 'services', 'office', 'u', 'of', 'illinoisurbana', 'phone', '2173335217', 'email', 'stankerruiucedu']]\n"
     ]
    }
   ],
   "source": [
    "# Function to remove punctuation, make everything lowercase, and tokenize\n",
    "def process_text(text):\n",
    "  \"\"\"Remove punctuation, lowercase, and tokenize text.\"\"\"\n",
    "  # TODO: check for special cases like \"I'll\"\n",
    "  text = \"\".join([char.lower() for char in text\n",
    "                  if char not in string.punctuation])\n",
    "  return word_tokenize(text)\n",
    "\n",
    "sentences = [process_text(document) for document in text_from_corpus.data]\n",
    "\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43312\n"
     ]
    }
   ],
   "source": [
    "# Train the natural language model with a window of 2 here\n",
    "news_model = Word2Vec(sentences, min_count=3, size=200, window=2)\n",
    "\n",
    "# Step 10 Print the length of the news model vocabulary\n",
    "print(len(news_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('population', 0.7684394121170044),\n",
       " ('crime', 0.7567795515060425),\n",
       " ('growth', 0.7356275320053101),\n",
       " ('operation', 0.7351930141448975),\n",
       " ('violation', 0.7330811619758606),\n",
       " ('european', 0.7258375883102417),\n",
       " ('degree', 0.7233099937438965),\n",
       " ('nuclear', 0.7212082147598267),\n",
       " ('capital', 0.7208678722381592),\n",
       " ('military', 0.7201747894287109)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 11 Find what's most similar to a chosen word here.\n",
    "news_model.wv.most_similar('mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anymore', 0.7359534502029419),\n",
       " ('bullshit', 0.7355056405067444),\n",
       " ('fortunately', 0.7311413288116455),\n",
       " ('fantasy', 0.7249035835266113),\n",
       " ('strikes', 0.7223988771438599),\n",
       " ('forgiveness', 0.7183439135551453),\n",
       " ('expressing', 0.7166538238525391),\n",
       " ('stiff', 0.714094877243042),\n",
       " ('praise', 0.7107727527618408),\n",
       " ('torment', 0.7060607671737671)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 11 Try more words\n",
    "news_model.wv.most_similar('humor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkzdOZm38yQ_"
   },
   "source": [
    "### Stretch Goals:\n",
    "\n",
    "1) Use Doc2Vec to train a model on your dataset, and then provide model with a new document and let it find similar documents.\n",
    "\n",
    "2) Download the pre-trained word vectors from Google. Access the pre-trained vectors via the following link: https://code.google.com/archive/p/word2vec\n",
    "\n",
    "Load the pre-trained word vectors and train the Word2vec model\n",
    "\n",
    "Examine the first 100 keys or words of the vocabulary\n",
    "\n",
    "Outputs the vector representation for a select set of words - the words can be of your choice\n",
    "\n",
    "Examine the similarity between words - the words can be of your choice\n",
    "\n",
    "For example:\n",
    "\n",
    "model.similarity('house', 'bungalow')\n",
    "\n",
    "model.similarity('house', 'umbrella')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gakr5rP76IAJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_424_Word_Embeddings_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
